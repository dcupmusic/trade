{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import math, time\n",
    "import itertools\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 800\n",
    "vbt.settings['plotting']['layout']['height'] = 400\n",
    "\n",
    "# #hparams\n",
    "timestep = 20\n",
    "# # Update these dimensions based on your dataset\n",
    "\n",
    "# hidden_dim = 32\n",
    "# num_layers = 2\n",
    "\n",
    "# num_epochs = 200\n",
    "# learning_rate=0.01\n",
    "# step_size=30\n",
    "# gamma=0.9\n",
    "\n",
    "# dropout_rate=0.2\n",
    "# print_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "df['signal'] = df['signal'].replace({'SignalNone': 1, 'SignalLong': 2, 'SignalShort': 0})\n",
    "df.fillna(method='pad');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)\n",
    "features = data.run(\"talib\", mavp=vbt.run_arg_dict(periods=14))\n",
    "data.data['symbol'] = pd.concat([data.data['symbol'], features], axis=1)\n",
    "data.data['symbol'].drop(['Open', 'High', 'Low'], axis=1, inplace=True)\n",
    "# This will drop columns from the DataFrame where all values are NaN\n",
    "data.data['symbol'] = data.data['symbol'].dropna(axis=1, how='all')\n",
    "\n",
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "data.data['symbol'] = data.data['symbol'].dropna()\n",
    "predictor_list = data.data['symbol'].drop('signal', axis=1).columns.tolist()\n",
    "\n",
    "\n",
    "X = data.data['symbol'][predictor_list]\n",
    "\n",
    "y = data.data['symbol']['signal']\n",
    "\n",
    "X.columns = X.columns.astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# First, split your data into a training+validation set and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Then, split the training+validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2)  # 0.2 here means 20% of the original data, or 25% of the training+validation set\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.fit_transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data, sequence_length):\n",
    "    sequences = []\n",
    "    data_len = len(input_data)\n",
    "    for i in range(data_len - sequence_length):\n",
    "        seq = input_data[i:(i + sequence_length)]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = create_sequences(X_train_scaled, timestep)\n",
    "X_test_list = create_sequences(X_test_scaled, timestep)\n",
    "X_val_list = create_sequences(X_val_scaled, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_seq_ar = y_train[timestep:]\n",
    "y_test_seq_ar = y_test[timestep:]\n",
    "y_val_seq_ar = y_val[timestep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ar = np.array(X_train_list)\n",
    "x_test_ar = np.array(X_test_list)  \n",
    "x_val_ar = np.array(X_val_list)  \n",
    "\n",
    "y_train_seq = np.array(y_train_seq_ar)\n",
    "y_test_seq = np.array(y_test_seq_ar)\n",
    "y_val_seq = np.array(y_val_seq_ar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(x_train_ar, dtype=torch.float32) # .to(device)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(x_test_ar, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(x_val_ar, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "\n",
    "if isinstance(y_train_seq, torch.Tensor):\n",
    "    y_train_seq_np = y_train_seq.cpu().numpy()\n",
    "else:\n",
    "    y_train_seq_np = y_train_seq  # Assuming y_train_seq is already a numpy array or similar\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_seq_np), y=y_train_seq_np)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# Move class weights to the same device as your model and data\n",
    "class_weights_tensor = class_weights_tensor.to(device)  # device could be 'cpu' or 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-24 15:32:16,551] A new study created in memory with name: no-name-f2d634c2-651f-46d4-8de1-4f13cba7830d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-02-24 15:33:19,643] Trial 0 finished with value: 3.855517625808716 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'lr': 0.0008057052762749867, 'step_size': 30, 'gamma': 0.9820801163811199, 'dropout_rate': 0.1439361557306276}. Best is trial 0 with value: 3.855517625808716.\n",
      "[I 2024-02-24 15:33:32,066] Trial 1 finished with value: 7.601255416870117 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'lr': 0.03742393108033467, 'step_size': 18, 'gamma': 0.8651227660612031, 'dropout_rate': 0.14385066918291542}. Best is trial 0 with value: 3.855517625808716.\n",
      "[I 2024-02-24 15:33:55,240] Trial 2 finished with value: 8.367830276489258 and parameters: {'hidden_dim': 64, 'num_layers': 1, 'lr': 0.021046257584648502, 'step_size': 55, 'gamma': 0.9891089814163857, 'dropout_rate': 0.16095509009397027}. Best is trial 0 with value: 3.855517625808716.\n",
      "[I 2024-02-24 15:34:07,058] Trial 3 finished with value: 1.1158785820007324 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'lr': 5.873276117857861e-05, 'step_size': 53, 'gamma': 0.9835845819104614, 'dropout_rate': 0.3856432026445499}. Best is trial 3 with value: 1.1158785820007324.\n",
      "[I 2024-02-24 15:34:18,553] Trial 4 finished with value: 6.832634925842285 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'lr': 0.06398819449295634, 'step_size': 76, 'gamma': 0.9141309166549905, 'dropout_rate': 0.20904579076361432}. Best is trial 3 with value: 1.1158785820007324.\n",
      "[I 2024-02-24 15:34:36,388] Trial 5 finished with value: 2.2422547340393066 and parameters: {'hidden_dim': 32, 'num_layers': 2, 'lr': 0.0008817973276671636, 'step_size': 78, 'gamma': 0.8941956115482609, 'dropout_rate': 0.3587242206154292}. Best is trial 3 with value: 1.1158785820007324.\n",
      "[I 2024-02-24 15:35:00,821] Trial 6 finished with value: 7.278936862945557 and parameters: {'hidden_dim': 32, 'num_layers': 3, 'lr': 0.008061829042542754, 'step_size': 26, 'gamma': 0.8623692617859736, 'dropout_rate': 0.3445765258785317}. Best is trial 3 with value: 1.1158785820007324.\n",
      "[I 2024-02-24 15:43:51,227] Trial 7 finished with value: 1.0969551801681519 and parameters: {'hidden_dim': 64, 'num_layers': 3, 'lr': 2.21015078595355e-05, 'step_size': 62, 'gamma': 0.9877575829732127, 'dropout_rate': 0.21386160007020788}. Best is trial 7 with value: 1.0969551801681519.\n",
      "[I 2024-02-24 15:45:48,374] Trial 8 finished with value: 5.687515735626221 and parameters: {'hidden_dim': 64, 'num_layers': 2, 'lr': 0.002329894386362853, 'step_size': 93, 'gamma': 0.8660089453123334, 'dropout_rate': 0.10931291855813514}. Best is trial 7 with value: 1.0969551801681519.\n",
      "[I 2024-02-24 15:46:16,595] Trial 9 finished with value: 8.359265327453613 and parameters: {'hidden_dim': 64, 'num_layers': 1, 'lr': 0.02609348241801102, 'step_size': 27, 'gamma': 0.963070681638691, 'dropout_rate': 0.35016471764896273}. Best is trial 7 with value: 1.0969551801681519.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: {'hidden_dim': 64, 'num_layers': 3, 'lr': 2.21015078595355e-05, 'step_size': 62, 'gamma': 0.9877575829732127, 'dropout_rate': 0.21386160007020788}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X_train_tensor and y_train_tensor are your input and output training tensors\n",
    "# Make sure X_train_tensor and y_test_tensor are already tensors\n",
    "\n",
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "num_epochs = 100\n",
    "input_dim = 175  # Number of features\n",
    "output_dim = 3  # Number of classes\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        # The input dimension is twice the hidden_dim because it's bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Concatenate the hidden states from both directions\n",
    "        out = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        \n",
    "        # Pass the concatenated hidden states to the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [16, 32, 64])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    step_size = trial.suggest_int('step_size', 10, 100)\n",
    "    gamma = trial.suggest_float('gamma', 0.85, 0.99)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.4)\n",
    "\n",
    "    # Initialize model and move it to the MPS device\n",
    "    model = BiLSTMClassifier(input_dim=X_train_tensor.shape[2], hidden_dim=hidden_dim, num_layers=num_layers, output_dim=len(np.unique(y_train_tensor.cpu().numpy())), dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    X_train_tensor_gpu = X_train_tensor.to(device)\n",
    "    y_train_tensor_gpu = y_train_tensor.to(device)\n",
    "    X_val_tensor_gpu = X_val_tensor.to(device)\n",
    "    y_val_tensor_gpu = y_val_tensor.to(device)\n",
    "    X_test_tensor_gpu = X_test_tensor.to(device)\n",
    "    y_test_tensor_gpu = y_test_tensor.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # use a small number of epochs for demonstration\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor_gpu)\n",
    "        loss = criterion(output, y_train_tensor_gpu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # Validation step\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_output = model(X_val_tensor_gpu)\n",
    "                val_loss = criterion(val_output, y_val_tensor_gpu)\n",
    "            model.train()\n",
    "\n",
    "    # model.eval()\n",
    "    # with torch.no_grad(): \n",
    "        # predictions = model(X_test_tensor_gpu)\n",
    "        # test_loss = criterion(predictions, y_test_tensor_gpu)\n",
    "        \n",
    "    # return test_loss.item()\n",
    "    \n",
    "    return val_loss.item()\n",
    "\n",
    "# Before running the study, ensure your data tensors are on the CPU as Optuna will handle moving them to the GPU\n",
    "X_train_tensor = X_train_tensor.cpu()\n",
    "y_train_tensor = y_train_tensor.cpu()\n",
    "X_val_tensor = X_val_tensor.cpu()\n",
    "y_val_tensor = y_val_tensor.cpu()\n",
    "X_test_tensor = X_test_tensor.cpu()\n",
    "y_test_tensor = y_test_tensor.cpu()\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "print('Best trial:', study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust the figure size\n",
    "# plt.figure(figsize=(6, 3))\n",
    "\n",
    "# # Plot the training loss\n",
    "# plt.plot(hist, label=\"Training loss\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with torch.no_grad():\n",
    "#     y_test_pred = model(x_test)\n",
    "#     # Convert logits to probabilities\n",
    "#     probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "#     # Get the predicted class labels\n",
    "#     _, predicted_labels = torch.max(probabilities, 1)\n",
    "#     # Move the tensor to CPU and then convert to numpy\n",
    "#     predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "#     print(len(predicted_labels_numpy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_split = data.data['symbol'][-len(predicted_labels_numpy):].copy()\n",
    "# df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "# signal = df_split['signal']\n",
    "# entries = signal == 2\n",
    "# exits = signal == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf = vbt.Portfolio.from_signals(\n",
    "#     close=df_split.Close, \n",
    "#     long_entries=entries, \n",
    "#     long_exits=exits,\n",
    "#     size=100,\n",
    "#     size_type='value',\n",
    "#     init_cash='auto'\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf.plot(settings=dict(bm_returns=False)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pf.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# # Convert tensors to numpy arrays for use with Scikit-Learn\n",
    "# true_labels = y_test.cpu().numpy()\n",
    "# pred_labels = predicted_labels.cpu().numpy()\n",
    "\n",
    "# precision = precision_score(true_labels, pred_labels, average='macro')  # 'macro' for unweighted mean\n",
    "# recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "# f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "# conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "# print(f'F1 Score: {f1:.2f}')\n",
    "# print('Confusion Matrix:\\n', conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
