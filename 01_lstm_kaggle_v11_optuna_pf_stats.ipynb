{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "\n",
    "import math, time\n",
    "from math import sqrt\n",
    "import itertools\n",
    "\n",
    "import datetime\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 600\n",
    "vbt.settings['plotting']['layout']['height'] = 300\n",
    "\n",
    "# #hparams\n",
    "timestep = 20\n",
    "# # Update these dimensions based on your dataset\n",
    "\n",
    "# hidden_dim = 32\n",
    "# num_layers = 2\n",
    "\n",
    "# num_epochs = 200\n",
    "# learning_rate=0.01\n",
    "# step_size=30\n",
    "# gamma=0.9\n",
    "\n",
    "# dropout_rate=0.2\n",
    "# print_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df['signal'] = df['signal'].replace({'SignalNone': 1, 'SignalLong': 2, 'SignalShort': 0})\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)\n",
    "features = data.run(\"talib\", mavp=vbt.run_arg_dict(periods=14))\n",
    "data.data['symbol'] = pd.concat([data.data['symbol'], features], axis=1)\n",
    "data.data['symbol'].drop(['Open', 'High', 'Low'], axis=1, inplace=True)\n",
    "# This will drop columns from the DataFrame where all values are NaN\n",
    "data.data['symbol'] = data.data['symbol'].dropna(axis=1, how='all')\n",
    "\n",
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "data.data['symbol'] = data.data['symbol'].dropna()\n",
    "# predictor_list = data.data['symbol'].drop('signal', axis=1).columns.tolist()\n",
    "predictor_list = [('aroon', 'aroondown'), \n",
    "                  ('aroon', 'aroonup'), \n",
    "                  ('bop', 'real'), \n",
    "                  ('cci', 'real'), \n",
    "                  ('cdlbelthold', 'integer'), \n",
    "                  ('macd', 'macd'), \n",
    "                  ('midprice', 'real'), \n",
    "                  ('mom', 'real'), \n",
    "                  ('roc', 'real'), \n",
    "                  ('stochrsi', 'fastk'), \n",
    "                  ('willr', 'real')]\n",
    "\n",
    "\n",
    "X = data.data['symbol'][predictor_list]\n",
    "\n",
    "y = data.data['symbol']['signal']\n",
    "\n",
    "X.columns = X.columns.astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, split your data into a training+validation set and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Then, split the training+validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, shuffle=False)  # 0.2 here means 20% of the original data, or 25% of the training+validation set\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.fit_transform(X_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sequences(input_data, sequence_length):\n",
    "    sequences = []\n",
    "    data_len = len(input_data)\n",
    "    for i in range(data_len - sequence_length):\n",
    "        seq = input_data[i:(i + sequence_length)]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_list = create_sequences(X_train_scaled, timestep)\n",
    "X_test_list = create_sequences(X_test_scaled, timestep)\n",
    "X_val_list = create_sequences(X_val_scaled, timestep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_seq_ar = y_train[timestep:]\n",
    "y_test_seq_ar = y_test[timestep:]\n",
    "y_val_seq_ar = y_val[timestep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_ar = np.array(X_train_list)\n",
    "x_test_ar = np.array(X_test_list)  \n",
    "x_val_ar = np.array(X_val_list)  \n",
    "\n",
    "y_train_seq = np.array(y_train_seq_ar).astype(int)\n",
    "y_test_seq = np.array(y_test_seq_ar).astype(int)\n",
    "y_val_seq = np.array(y_val_seq_ar).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(x_train_ar, dtype=torch.float32) # .to(device)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(x_test_ar, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(x_val_ar, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "\n",
    "if isinstance(y_train_seq, torch.Tensor):\n",
    "    y_train_seq_np = y_train_seq.cpu().numpy()\n",
    "else:\n",
    "    y_train_seq_np = y_train_seq  # Assuming y_train_seq is already a numpy array or similar\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_seq_np), y=y_train_seq_np)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# Move class weights to the same device as your model and data\n",
    "class_weights_tensor = class_weights_tensor.to(device)  # device could be 'cpu' or 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming X_train_tensor and y_train_tensor are your input and output training tensors\n",
    "# Make sure X_train_tensor and y_test_tensor are already tensors\n",
    "\n",
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Bidirectional LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        # The input dimension is twice the hidden_dim because it's bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, batch_size, self.hidden_dim).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Concatenate the hidden states from both directions\n",
    "        out = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        \n",
    "        # Pass the concatenated hidden states to the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 50\n",
    "num_trials=50\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [16, 32, 64])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    step_size = trial.suggest_int('step_size', 10, 100)\n",
    "    gamma = trial.suggest_float('gamma', 0.85, 0.99)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.4)\n",
    "\n",
    "    # Initialize model and move it to the MPS device\n",
    "    model = BiLSTMClassifier(input_dim=X_train_tensor.shape[2], hidden_dim=hidden_dim, num_layers=num_layers, output_dim=len(np.unique(y_train_tensor.cpu().numpy())), dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    X_train_tensor_gpu = X_train_tensor.to(device)\n",
    "    y_train_tensor_gpu = y_train_tensor.to(device)\n",
    "    X_val_tensor_gpu = X_val_tensor.to(device)\n",
    "    y_val_tensor_gpu = y_val_tensor.to(device)\n",
    "    X_test_tensor_gpu = X_test_tensor.to(device)\n",
    "    y_test_tensor_gpu = y_test_tensor.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # use a small number of epochs for demonstration\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_tensor_gpu)\n",
    "        loss = criterion(output, y_train_tensor_gpu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_test_tensor_gpu)\n",
    "        probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "        _, predicted_labels = torch.max(probabilities, 1)\n",
    "        predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Use predicted labels to simulate a trading strategy\n",
    "    df_split = data.data['symbol'][-len(predicted_labels_numpy):].copy()\n",
    "    df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "    signal = df_split['signal']\n",
    "    entries = signal == 2\n",
    "    exits = signal == 0\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=df_split.Close, \n",
    "        long_entries=entries, \n",
    "        long_exits=exits,\n",
    "        size=100,\n",
    "        size_type='value',\n",
    "        init_cash='auto'\n",
    "    )\n",
    "    pf.plot({\"orders\", \"cum_returns\"}, settings=dict(bm_returns=False)).show()\n",
    "    stats = pf.stats()\n",
    "    total_return = stats['Total Return [%]']\n",
    "    max_drawdown = stats['Max Drawdown [%]']\n",
    "    orders = stats['Total Orders']\n",
    "\n",
    "    if orders < 5:\n",
    "        print(f\"Only {orders} trades were made\")\n",
    "        total_return = 0.0\n",
    "    my_score = total_return - max_drawdown\n",
    "    # Return the negative total return as the objective to maximize it\n",
    "    return my_score\n",
    "\n",
    "# Before running the study, ensure your data tensors are on the CPU as Optuna will handle moving them to the GPU\n",
    "X_train_tensor = X_train_tensor.cpu()\n",
    "y_train_tensor = y_train_tensor.cpu()\n",
    "X_val_tensor = X_val_tensor.cpu()\n",
    "y_val_tensor = y_val_tensor.cpu()\n",
    "X_test_tensor = X_test_tensor.cpu()\n",
    "y_test_tensor = y_test_tensor.cpu()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "\n",
    "print('Best trial:', study.best_trial.params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
