{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "import random\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df_filtered = df[df['signal'] != 'SignalNone']\n",
    "\n",
    "# Iterate through the DataFrame and adjust the signals\n",
    "for i in range(1, len(df_filtered)):\n",
    "    current_signal = df_filtered.iloc[i]['signal']\n",
    "    previous_signal = df_filtered.iloc[i - 1]['signal']\n",
    "    current_close = df_filtered.iloc[i]['Close']\n",
    "    previous_close = df_filtered.iloc[i - 1]['Close']\n",
    "    \n",
    "    if current_signal == previous_signal:\n",
    "        if current_signal == 'SignalLong' and previous_close > current_close:\n",
    "            df_filtered.iloc[i - 1, df_filtered.columns.get_loc('signal')] = 'SignalNone'\n",
    "        elif current_signal != 'SignalLong' and previous_close < current_close:\n",
    "            df_filtered.iloc[i - 1, df_filtered.columns.get_loc('signal')] = 'SignalNone'\n",
    "        else:\n",
    "            df_filtered.iloc[i, df_filtered.columns.get_loc('signal')] = 'SignalNone'\n",
    "\n",
    "\n",
    "df.update(df_filtered)\n",
    "\n",
    "if binary:\n",
    "    # Assuming df is your DataFrame\n",
    "    previous_signal = None  # Initialize a variable to keep track of the previous non-\"SignalNone\" value\n",
    "\n",
    "    for i in range(len(df)):\n",
    "        if df.iloc[i, df_filtered.columns.get_loc('signal')] == \"SignalNone\" and previous_signal is not None:\n",
    "            df.iloc[i, df_filtered.columns.get_loc('signal')] = previous_signal  # Replace \"SignalNone\" with the previous signal\n",
    "        elif df.iloc[i, df_filtered.columns.get_loc('signal')] != \"SignalNone\":\n",
    "            previous_signal = df.iloc[i, df_filtered.columns.get_loc('signal')]  # Update the previous signal to the current one if it's not \"SignalNone\"\n",
    "\n",
    "    df = df.loc[df['signal'] != 'SignalNone']\n",
    "\n",
    "df['signal'] = df['signal'].replace({'SignalLong': 1, 'SignalShort': 0, 'SignalNone': 2})\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)\n",
    "features = data.run(\"talib\", mavp=vbt.run_arg_dict(periods=14))\n",
    "data.data['symbol'] = pd.concat([data.data['symbol'], features], axis=1)\n",
    "data.data['symbol'].drop(['Open', 'High', 'Low'], axis=1, inplace=True)\n",
    "# This will drop columns from the DataFrame where all values are NaN\n",
    "data.data['symbol'] = data.data['symbol'].dropna(axis=1, how='all')\n",
    "\n",
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "data.data['symbol'] = data.data['symbol'].dropna()\n",
    "predictor_list = data.data['symbol'].drop('signal', axis=1).columns.tolist()\n",
    "\n",
    "\n",
    "X = data.data['symbol'][predictor_list]\n",
    "\n",
    "y = data.data['symbol']['signal']\n",
    "\n",
    "X.columns = X.columns.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set\n",
    "# Assuming X is a DataFrame or a NumPy array\n",
    "indices = np.arange(X.shape[0])\n",
    "\n",
    "# First, split your data into a training+validation set and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test, indices_train_val, indices_test = train_test_split(X, y, indices, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Then, split the training+validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(X_train_val, y_train_val, indices_train_val, test_size=0.2, shuffle=False)  # 0.2 here means 20% of the original data, or 25% of the training+validation set\n",
    "\n",
    "# Now, `indices_val` holds the indices of your original dataset that were used for the validation set.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 20\n",
    "\n",
    "def create_sequences(input_data, timestep):\n",
    "    sequences = []\n",
    "    data_len = len(input_data)\n",
    "    for i in range(data_len - timestep):\n",
    "        seq = input_data[i:(i + timestep)]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "X_train_list = create_sequences(X_train_scaled, timestep)\n",
    "X_val_list = create_sequences(X_val_scaled, timestep)\n",
    "X_test_list = create_sequences(X_test_scaled, timestep)\n",
    "y_train_seq_ar = y_train[timestep:]\n",
    "y_val_seq_ar = y_val[timestep:]\n",
    "y_test_seq_ar = y_test[timestep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "x_train_ar = np.array(X_train_list)\n",
    "y_train_seq = np.array(y_train_seq_ar).astype(int)\n",
    "x_val_ar = np.array(X_val_list)  \n",
    "y_val_seq = np.array(y_val_seq_ar).astype(int)\n",
    "x_test_ar = np.array(X_test_list)  \n",
    "y_test_seq = np.array(y_test_seq_ar).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(x_train_ar, dtype=torch.float32)\n",
    "X_val_tensor = torch.tensor(x_val_ar, dtype=torch.float32)\n",
    "X_test_tensor = torch.tensor(x_test_ar, dtype=torch.float32)\n",
    "\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.long)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.long)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.long)\n",
    "\n",
    "if binary:\n",
    "    y_train_tensor = torch.tensor(y_train_seq, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val_seq, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test_seq, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if binary:\n",
    "    class_weights_tensor = torch.tensor([1.0, 1.0, 1.0])\n",
    "else:\n",
    "    if isinstance(y_train_seq, torch.Tensor):\n",
    "        y_train_seq_np = y_train_seq.cpu().numpy()\n",
    "    else:\n",
    "        y_train_seq_np = y_train_seq  # Assuming y_train_seq is already a numpy array or similar\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_seq_np), y=y_train_seq_np)\n",
    "    # Decrease the weight of the '2' class\n",
    "    decrease_factor = 0.5  # Adjust this factor as needed\n",
    "    class_weights[2] *= decrease_factor\n",
    "\n",
    "    class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "    # Move class weights to the same device as your model and data\n",
    "    class_weights_tensor = class_weights_tensor.to(device)  # device could be 'cpu' or 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output shape: [batch_size, seq_length, hidden_dim]\n",
    "        weights = torch.tanh(self.linear(lstm_output))\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        \n",
    "        # Context vector with weighted sum\n",
    "        context = weights * lstm_output\n",
    "        context = torch.sum(context, dim=1)\n",
    "        return context, weights\n",
    "\n",
    "class BiLSTMClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate):\n",
    "        super(BiLSTMClassifierWithAttention, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Batch Normalization Layer for Conv1d\n",
    "        self.bn_conv1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = Attention(hidden_dim * 2)  # For bidirectional LSTM\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # Adjusted for attention context vector\n",
    "        \n",
    "        # Batch Normalization Layer for FC1\n",
    "        self.bn_fc1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
    "        \n",
    "        # Additional Dropout for the fully connected layer\n",
    "        self.dropout_fc = nn.Dropout(dropout_rate / 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape x for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn_conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Reshape back for LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Applying attention mechanism to LSTM outputs\n",
    "        context, _ = self.attention(out)\n",
    "        \n",
    "        # Fully connected layers using the context vector from attention\n",
    "        out = self.fc1(context)\n",
    "        out = self.bn_fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout_fc(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest(model, X_val_selected_gpu):\n",
    "    # predictions and backtest\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_val_selected_gpu)\n",
    "        if binary:\n",
    "            probabilities = torch.sigmoid(y_test_pred).squeeze()\n",
    "            predicted_labels = (probabilities > 0.5).long()\n",
    "        else:    \n",
    "            probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "            _, predicted_labels = torch.max(probabilities, 1)\n",
    "        predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Use predicted labels to simulate a trading strategy\n",
    "    adjusted_indices_val = indices_val[timestep:]\n",
    "    adjusted_indices_test = indices_test[timestep:] \n",
    "\n",
    "    df_split = data.data['symbol'].iloc[adjusted_indices_val].copy()\n",
    "    df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "    signal = df_split['signal']\n",
    "    entries = signal == 1\n",
    "    exits = signal == 0\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=df_split.Close, \n",
    "        long_entries=entries, \n",
    "        short_entries=exits,\n",
    "        size=100,\n",
    "        size_type='value',\n",
    "        init_cash='auto'\n",
    "    )\n",
    "    stats = pf.stats()\n",
    "    total_return = round(stats['Total Return [%]'], 2)\n",
    "    print(f\"Total Return: {total_return}%\")\n",
    "    vbt.settings.set_theme('dark')\n",
    "    vbt.settings['plotting']['layout']['width'] = 600\n",
    "    vbt.settings['plotting']['layout']['height'] = 200\n",
    "    pf.plot({\"orders\", \"cum_returns\"}).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_multi_with_metrics(model, criterion, X_val, y_val, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(X_val)\n",
    "        predicted_probs = torch.softmax(output, dim=1)\n",
    "        predictions = torch.argmax(predicted_probs, dim=1)  # Get the index of the max log-probability as the prediction\n",
    "        \n",
    "        loss = criterion(output, y_val)  # Ensure y_val is of dtype long and contains class indices\n",
    "\n",
    "        # Convert to CPU and numpy for sklearn metrics\n",
    "        predictions_np = predictions.cpu().numpy()\n",
    "        y_val_np = y_val.cpu().numpy()\n",
    "\n",
    "        accuracy = accuracy_score(y_val_np, predictions_np)\n",
    "        precision = precision_score(y_val_np, predictions_np, average='weighted')\n",
    "        recall = recall_score(y_val_np, predictions_np, average='weighted')\n",
    "        f1 = f1_score(y_val_np, predictions_np, average='weighted')\n",
    "\n",
    "    model.train()  # Set back to train mode\n",
    "    return {\n",
    "        'loss': round(loss.item(), 4),\n",
    "        'accuracy': round(accuracy, 4),\n",
    "        'precision': round(precision, 4),\n",
    "        'recall': round(recall, 4),\n",
    "        'f1': round(f1, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "def validate_binary_with_metrics(model, criterion, X_val, y_val, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(X_val)\n",
    "        predicted_probs = torch.sigmoid(output)\n",
    "        predictions = (predicted_probs > 0.5).float()  # Apply threshold to get binary predictions\n",
    "        loss = criterion(output, y_val.view_as(output))\n",
    "\n",
    "        # Convert to CPU and numpy for sklearn metrics\n",
    "        predictions_np = predictions.cpu().numpy()\n",
    "        y_val_np = y_val.cpu().numpy()\n",
    "        predicted_probs_np = predicted_probs.cpu().numpy()\n",
    "\n",
    "        accuracy = (predictions.view_as(y_val) == y_val).sum().item() / len(y_val)\n",
    "        precision = precision_score(y_val_np, predictions_np)\n",
    "        recall = recall_score(y_val_np, predictions_np)\n",
    "        f1 = f1_score(y_val_np, predictions_np)\n",
    "        auc = roc_auc_score(y_val_np, predicted_probs_np)  # Use probabilities for AUC\n",
    "\n",
    "    model.train()  # Set back to train mode\n",
    "    return {\n",
    "        'loss': round(loss.item(),4),\n",
    "        'accuracy': round(accuracy,4),\n",
    "        'precision': round(precision,4),\n",
    "        'recall': round(recall,4),\n",
    "        'f1': round(f1,4),\n",
    "        'auc': round(auc,4)\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_financials(model, X_val_selected_gpu):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_val_selected_gpu)\n",
    "        probabilities = torch.sigmoid(y_test_pred).squeeze()  # Apply sigmoid to convert logits to probabilities\n",
    "        predicted_labels = (probabilities > 0.5).long()  # Threshold probabilities to get binary predictions\n",
    "        predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Use predicted labels to simulate a trading strategy\n",
    "    adjusted_indices_val = indices_val[timestep:]\n",
    "    adjusted_indices_test = indices_test[timestep:] \n",
    "    \n",
    "    df_split = data.data['symbol'].iloc[adjusted_indices_val].copy()\n",
    "    df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "    signal = df_split['signal']\n",
    "    entries = signal == 1\n",
    "    exits = signal == 0\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=df_split.Close, \n",
    "        long_entries=entries, \n",
    "        short_entries=exits,\n",
    "        size=100,\n",
    "        size_type='value',\n",
    "        init_cash='auto'\n",
    "    )\n",
    "    stats = pf.stats()\n",
    "    total_return = stats['Total Return [%]']\n",
    "    orders = stats['Total Orders']\n",
    "    calmer_ratio = stats['Calmar Ratio']\n",
    "    \n",
    "    model.train()\n",
    "    return {\n",
    "        \"orders\": orders,\n",
    "        \"calmer_returns\": (calmer_ratio+total_return)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vals(epoch_nums_1, accuracy, precision, recall, f1):\n",
    "    validation_metrics_fig = go.Figure()\n",
    "    # validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=val_loss, mode='lines+markers', name='val_loss'))\n",
    "    validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=accuracy, mode='lines+markers', name='accuracy'))\n",
    "    validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=precision, mode='lines+markers', name='precision'))\n",
    "    validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=recall, mode='lines+markers', name='recall'))\n",
    "    validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=f1, mode='lines+markers', name='f1'))\n",
    "    validation_metrics_fig.update_layout(\n",
    "        template='plotly_dark',\n",
    "        autosize=False,\n",
    "        width=700,  # Set the width of the figure\n",
    "        height=150,  # Set the height of the figure\n",
    "        title='val metrics over epochs',\n",
    "        title_font_size=10,\n",
    "        margin=dict(l=5, r=5, b=5, t=30, pad=5),\n",
    "        legend=dict(\n",
    "            font=dict(\n",
    "                size=5)\n",
    "        )\n",
    "    )       \n",
    "    validation_metrics_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_returns(epoch_nums_1, calmer_return):\n",
    "    return_metrics_fig = go.Figure()\n",
    "\n",
    "    return_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=calmer_return, mode='lines+markers', name='calmer returns'))\n",
    "\n",
    "    return_metrics_fig.update_layout(\n",
    "        template='plotly_dark',\n",
    "        autosize=False,\n",
    "        width=700,  # Set the width of the figure\n",
    "        height=150,  # Set the height of the figure\n",
    "        title='calmer ratio + returns over epochs',  # You can set the title directly here\n",
    "        title_font_size=10,\n",
    "        margin=dict(l=5, r=5, b=5, t=30, pad=5),\n",
    "        legend=dict(\n",
    "            font=dict(\n",
    "                size=5)\n",
    "        )\n",
    "    )\n",
    "    return_metrics_fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "learning_rate=0.5\n",
    "step_size=10\n",
    "gamma=0.8\n",
    "dropout_rate=0.2\n",
    "\n",
    "variance_threshold = 3\n",
    "rolling_window_size = 15\n",
    "num_epochs = 200\n",
    "print_epochs = 5\n",
    "\n",
    "num_trials = 3\n",
    "\n",
    "def objective(trial):\n",
    "    \n",
    "    random.seed(42)\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "\n",
    "\n",
    "    feature_idx = 89 # trial.suggest_int('feature_idx', 0, X_train_tensor.shape[2] - 1)\n",
    "    X_train_selected = X_train_tensor[:, :, feature_idx:feature_idx+1]\n",
    "    X_val_selected = X_val_tensor[:, :, feature_idx:feature_idx+1]\n",
    "    # X_test_selected = X_test_tensor[:, :, feature_idx:feature_idx+1]\n",
    "    \n",
    "    # Move tensors to the MPS device\n",
    "    X_train_selected_gpu = X_train_selected.float().to(device)\n",
    "    X_val_selected_gpu = X_val_selected.float().to(device)\n",
    "    y_train_tensor_gpu = y_train_tensor.long().to(device)\n",
    "    y_val_tensor_gpu = y_val_tensor.long().to(device)\n",
    "    \n",
    "    out_dims = len(np.unique(y_train_tensor.cpu().numpy()))\n",
    "    if binary:\n",
    "        out_dims = 1\n",
    "        y_train_tensor_gpu = y_train_tensor.float().to(device)\n",
    "        y_val_tensor_gpu = y_val_tensor.float().to(device)\n",
    "    \n",
    "    # Create the model with bidirectional LSTM\n",
    "    model = BiLSTMClassifierWithAttention(input_dim=X_train_selected.shape[-1], hidden_dim=hidden_dim, num_layers=num_layers, output_dim=out_dims, dropout_rate=dropout_rate).to(device)\n",
    "    optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimiser, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    if binary:\n",
    "        criterion = nn.BCEWithLogitsLoss()\n",
    "        y_val_tensor_gpu = y_val_tensor_gpu.unsqueeze(1)\n",
    "        \n",
    "\n",
    "\n",
    "    rolling_window = deque(maxlen=rolling_window_size)\n",
    "    epoch_nums_1 = []\n",
    "    calmer_return = []\n",
    "    val_loss = []\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []        \n",
    "   \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs): \n",
    "        optimiser.zero_grad()   \n",
    "        output = model(X_train_selected_gpu)\n",
    "        output = torch.squeeze(output)  # This removes the extra dimension\n",
    "        loss = criterion(output, y_train_tensor_gpu)  # Ensure y_train is of type torch.long\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        if epoch % print_epochs == 0:  # Adjust as needed\n",
    "            financial_results = validate_financials(model, X_val_selected_gpu)\n",
    "            validation_results = validate_multi_with_metrics(model, criterion, X_val_selected_gpu, y_val_tensor_gpu, device)\n",
    "            \n",
    "            if binary:\n",
    "                validation_results = validate_binary_with_metrics(model, criterion, X_val_selected_gpu, y_val_tensor_gpu, device)\n",
    "            epoch_nums_1.append(epoch)\n",
    "            accuracy.append(validation_results['accuracy'])\n",
    "            val_loss.append(validation_results['loss'])\n",
    "            precision.append(validation_results['precision'])\n",
    "            recall.append(validation_results['recall'])\n",
    "            f1.append(validation_results['f1'])\n",
    "            calmer_return.append(financial_results['calmer_returns'])\n",
    "            rolling_window.append(financial_results['calmer_returns'])\n",
    "            \n",
    "            if len(rolling_window) == rolling_window_size:\n",
    "                variance = np.var(list(rolling_window))\n",
    "                \n",
    "                if variance < variance_threshold:\n",
    "                    print(f\"Early stopping triggered epoch {epoch}. Variance is below the threshold.\")\n",
    "                    break\n",
    "                if len(calmer_return) >= rolling_window_size and all(x < 0 for x in calmer_return[-rolling_window_size:]):\n",
    "                    print(f\"Early stopping triggered epoch {epoch}. The last {rolling_window_size} returns are negative.\")\n",
    "                    break\n",
    "            \n",
    "\n",
    "    plot_vals(epoch_nums_1, accuracy, precision, recall, f1)\n",
    "    plot_returns(epoch_nums_1, calmer_return)\n",
    "    backtest(model, X_val_selected_gpu)\n",
    "    return loss.item()\n",
    "\n",
    "sampler = optuna.samplers.TPESampler(seed=42)\n",
    "study = optuna.create_study(direction='minimize', sampler=sampler)\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "\n",
    "print('Best trial:', study.best_trial.params)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
