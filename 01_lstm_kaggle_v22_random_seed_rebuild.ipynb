{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "import random\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Seed for Python's random module\n",
    "random.seed(42)\n",
    "\n",
    "# Seed for NumPy's random number generator\n",
    "np.random.seed(42)\n",
    "\n",
    "# Seed for PyTorch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Additional settings for multi-threaded reproducibility\n",
    "torch.use_deterministic_algorithms(True)  # This is a newer way compared to 'torch.backends.cudnn.deterministic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df_filtered = df[df['signal'] != 'SignalNone']\n",
    "\n",
    "# Iterate through the DataFrame and adjust the signals\n",
    "for i in range(1, len(df_filtered)):\n",
    "    current_signal = df_filtered.iloc[i]['signal']\n",
    "    previous_signal = df_filtered.iloc[i - 1]['signal']\n",
    "    current_close = df_filtered.iloc[i]['Close']\n",
    "    previous_close = df_filtered.iloc[i - 1]['Close']\n",
    "    \n",
    "    if current_signal == previous_signal:\n",
    "        if current_signal == 'SignalLong' and previous_close > current_close:\n",
    "            df_filtered.iloc[i - 1, df_filtered.columns.get_loc('signal')] = 'SignalNone'\n",
    "        elif current_signal != 'SignalLong' and previous_close < current_close:\n",
    "            df_filtered.iloc[i - 1, df_filtered.columns.get_loc('signal')] = 'SignalNone'\n",
    "        else:\n",
    "            df_filtered.iloc[i, df_filtered.columns.get_loc('signal')] = 'SignalNone'\n",
    "\n",
    "\n",
    "df.update(df_filtered)\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# previous_signal = None  # Initialize a variable to keep track of the previous non-\"SignalNone\" value\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     if df.iloc[i, df_filtered.columns.get_loc('signal')] == \"SignalNone\" and previous_signal is not None:\n",
    "#         df.iloc[i, df_filtered.columns.get_loc('signal')] = previous_signal  # Replace \"SignalNone\" with the previous signal\n",
    "#     elif df.iloc[i, df_filtered.columns.get_loc('signal')] != \"SignalNone\":\n",
    "#         previous_signal = df.iloc[i, df_filtered.columns.get_loc('signal')]  # Update the previous signal to the current one if it's not \"SignalNone\"\n",
    "\n",
    "# df = df.loc[df['signal'] != 'SignalNone']\n",
    "\n",
    "df['signal'] = df['signal'].replace({'SignalLong': 2, 'SignalShort': 0, 'SignalNone': 1})\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)\n",
    "features = data.run(\"talib\", mavp=vbt.run_arg_dict(periods=14))\n",
    "data.data['symbol'] = pd.concat([data.data['symbol'], features], axis=1)\n",
    "data.data['symbol'].drop(['Open', 'High', 'Low'], axis=1, inplace=True)\n",
    "# This will drop columns from the DataFrame where all values are NaN\n",
    "data.data['symbol'] = data.data['symbol'].dropna(axis=1, how='all')\n",
    "\n",
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "data.data['symbol'] = data.data['symbol'].dropna()\n",
    "predictor_list = data.data['symbol'].drop('signal', axis=1).columns.tolist()\n",
    "\n",
    "\n",
    "X = data.data['symbol'][predictor_list]\n",
    "\n",
    "y = data.data['symbol']['signal']\n",
    "\n",
    "X.columns = X.columns.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate=0.01\n",
    "step_size=30\n",
    "gamma=0.9\n",
    "\n",
    "dropout_rate=0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set\n",
    "# Assuming X is a DataFrame or a NumPy array\n",
    "indices = np.arange(X.shape[0])\n",
    "\n",
    "# First, split your data into a training+validation set and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test, indices_train_val, indices_test = train_test_split(X, y, indices, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Then, split the training+validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(X_train_val, y_train_val, indices_train_val, test_size=0.2, shuffle=False)  # 0.2 here means 20% of the original data, or 25% of the training+validation set\n",
    "\n",
    "# Now, `indices_val` holds the indices of your original dataset that were used for the validation set.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 20\n",
    "\n",
    "def create_sequences(input_data, timestep):\n",
    "    sequences = []\n",
    "    data_len = len(input_data)\n",
    "    for i in range(data_len - timestep):\n",
    "        seq = input_data[i:(i + timestep)]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "X_train_list = create_sequences(X_train_scaled, timestep)\n",
    "X_val_list = create_sequences(X_val_scaled, timestep)\n",
    "X_test_list = create_sequences(X_test_scaled, timestep)\n",
    "y_train_seq_ar = y_train[timestep:]\n",
    "y_val_seq_ar = y_val[timestep:]\n",
    "y_test_seq_ar = y_test[timestep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_array = np.array(X_train_list)\n",
    "x_test_array = np.array(X_test_list)  \n",
    "\n",
    "y_train_array = np.array(y_train_seq_ar).astype(np.int64)\n",
    "y_test_array = np.array(y_test_seq_ar).astype(np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Move tensors to the MPS device\n",
    "x_train_gpu = torch.from_numpy(x_train_array).type(torch.Tensor).to(device)\n",
    "x_test_gpu = torch.from_numpy(x_test_array).type(torch.Tensor).to(device)\n",
    "y_train_gpu = torch.from_numpy(y_train_array).long().to(device)\n",
    "y_test_gpu = torch.from_numpy(y_test_array).long().to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming y_train is your target labels tensor for the training data\n",
    "# and it's already in the form of a 1D tensor of class indices (0 to C-1)\n",
    "\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = y_train  # Assuming y_train is already a numpy array\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Move class weights to the same device as your model and data\n",
    "class_weights_tensor = class_weights_tensor.to(device)  # device could be 'cpu' or 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "input_dim = X_train_list.shape[2]  # Number of features\n",
    "output_dim = 3  # Number of classes\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Bidirectional LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        # The input dimension is twice the hidden_dim because it's bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)  # times 2 for bidirectional\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)  # times 2 for bidirectional\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply dropout to the output of the LSTM\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Concatenate the hidden states from both directions\n",
    "        out = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        \n",
    "        # Pass the concatenated hidden states to the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create the model with bidirectional LSTM\n",
    "model = BiLSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "model.to(device)  # Move your model to the MPS device\n",
    "\n",
    "# Loss function, optimizer, and scheduler remain the same\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=step_size, gamma=gamma)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "print_epochs = 2\n",
    "# hist will track the loss for now\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "# Ensure your model is in training mode\n",
    "model.train()\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    # Make sure x_train and y_train are already on the correct device (GPU)\n",
    "    y_train_pred = model(x_train_gpu)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_train_pred, y_train_gpu.long())  # Ensure y_train is of type torch.long\n",
    "    if t % print_epochs == 0:  # Adjust logging frequency according to your preference\n",
    "        print(f\"Epoch {t}, Loss: {loss.item()}\")\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    # Zero gradients before backward pass\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Perform backward pass: compute gradients of the loss with respect to all the learnable parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters using the gradients and optimizer algorithm\n",
    "    optimiser.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Optional: Calculate and print accuracy or other metrics every few epochs\n",
    "    if t % print_epochs == 0:  # Adjust as needed\n",
    "        # Set the model to evaluation mode for accuracy calculation\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # No need to track gradients for validation\n",
    "            y_pred_tags = torch.argmax(torch.softmax(y_train_pred, dim=1), dim=1)\n",
    "            correct_preds = (y_pred_tags == y_train_gpu).float().sum()\n",
    "            accuracy = correct_preds / y_train_gpu.shape[0]\n",
    "            print(f'Epoch {t} Accuracy: {accuracy.item() * 100:.2f}%')\n",
    "        # Set the model back to training mode\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust the figure size\n",
    "# plt.figure(figsize=(6, 3))\n",
    "\n",
    "# # Plot the training loss\n",
    "# plt.plot(hist, label=\"Training loss\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_pred = model(x_test_gpu)\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "    # Get the predicted class labels\n",
    "    _, predicted_labels = torch.max(probabilities, 1)\n",
    "    # Move the tensor to CPU and then convert to numpy\n",
    "    predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "    print(len(predicted_labels_numpy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = data.data['symbol'][-len(predicted_labels_numpy):].copy()\n",
    "df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "signal = df_split['signal']\n",
    "entries = signal == 2\n",
    "exits = signal == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = vbt.Portfolio.from_signals(\n",
    "    close=df_split.Close, \n",
    "    long_entries=entries, \n",
    "    long_exits=exits,\n",
    "    size=100,\n",
    "    size_type='value',\n",
    "    init_cash='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 600\n",
    "vbt.settings['plotting']['layout']['height'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot({\"orders\", \"cum_returns\"}, settings=dict(bm_returns=False)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pf.stats()\n",
    "total_return = stats['Total Return [%]']\n",
    "orders = stats['Total Orders']\n",
    "print(\"Total Orders:\", orders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# # Convert tensors to numpy arrays for use with Scikit-Learn\n",
    "# true_labels = y_test.cpu().numpy()\n",
    "# pred_labels = predicted_labels.cpu().numpy()\n",
    "\n",
    "# precision = precision_score(true_labels, pred_labels, average='macro')  # 'macro' for unweighted mean\n",
    "# recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "# f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "# conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "# print(f'Precision: {precision:.2f}')\n",
    "# print(f'Recall: {recall:.2f}')\n",
    "# print(f'F1 Score: {f1:.2f}')\n",
    "# print('Confusion Matrix:\\n', conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
