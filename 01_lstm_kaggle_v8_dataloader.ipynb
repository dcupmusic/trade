{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "import math, time\n",
    "import itertools\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "\n",
    "# #hparams\n",
    "\n",
    "# # Update these dimensions based on your dataset\n",
    "\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate=0.01\n",
    "step_size=30\n",
    "gamma=0.9\n",
    "\n",
    "dropout_rate=0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "df['signal'] = df['signal'].replace({'SignalNone': 1, 'SignalLong': 2, 'SignalShort': 0})\n",
    "df.fillna(method='pad');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)\n",
    "features = data.run(\"talib\", mavp=vbt.run_arg_dict(periods=14))\n",
    "data.data['symbol'] = pd.concat([data.data['symbol'], features], axis=1)\n",
    "data.data['symbol'].drop(['Open', 'High', 'Low'], axis=1, inplace=True)\n",
    "# This will drop columns from the DataFrame where all values are NaN\n",
    "data.data['symbol'] = data.data['symbol'].dropna(axis=1, how='all')\n",
    "\n",
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "data.data['symbol'] = data.data['symbol'].dropna()\n",
    "\n",
    "# predictor_list = data.data['symbol'].drop('signal', axis=1).columns.tolist()\n",
    "predictor_list = [('cdlmorningdojistar', 'integer'), \n",
    "               ('cdlidentical3crows', 'integer'), \n",
    "               ('cdlhangingman', 'integer')]\n",
    "\n",
    "\n",
    "X = data.data['symbol'][predictor_list]\n",
    "\n",
    "y = data.data['symbol']['signal']\n",
    "\n",
    "X.columns = X.columns.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# First, split your data into a training+validation set and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Then, split the training+validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)  # 0.2 here means 20% of the original data, or 25% of the training+validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a scaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your data and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create a DataFrame from the scaled data with the same index and columns\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "\n",
    "# Fit the scaler to your data and transform\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# Create a DataFrame from the scaled data with the same index and columns\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming y_train is your target labels tensor for the training data\n",
    "# and it's already in the form of a 1D tensor of class indices (0 to C-1)\n",
    "\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = y_train  # Assuming y_train is already a numpy array\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Move class weights to the same device as your model and data\n",
    "class_weights_tensor = class_weights_tensor.to(device)  # device could be 'cpu' or 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 80\n",
    "\n",
    "def create_sequences(input_data, sequence_length):\n",
    "    sequences = []\n",
    "    data_len = len(input_data)\n",
    "    for i in range(data_len - sequence_length):\n",
    "        seq = input_data[i:(i + sequence_length)]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Assuming X_train_scaled_df and X_test_scaled_df are already scaled and are DataFrames\n",
    "X_train_list = create_sequences(X_train_scaled_df.values, timestep)\n",
    "X_test_list = create_sequences(X_test_scaled_df.values, timestep)\n",
    "\n",
    "\n",
    "y_train_list = create_sequences(y_train, timestep)\n",
    "y_test_list = create_sequences(y_test, timestep)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert your numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train_list, dtype=torch.float)\n",
    "y_train_tensor = torch.tensor(y_train_list, dtype=torch.long)  # Use torch.long for classification labels\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_list, dtype=torch.float)\n",
    "y_test_tensor = torch.tensor(y_test_list, dtype=torch.long)\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# Create DataLoaders\n",
    "batch_size = 20  # You can adjust the batch size\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)  # Typically no need to shuffle the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('x_train.shape = ',X_train_tensor.shape)\n",
    "print('x_test.shape = ',X_test_tensor.shape)\n",
    "print('y_train.shape = ',y_train_tensor.shape)\n",
    "print('y_test.shape = ',y_test_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "input_dim = X_train_list.shape[2]  # Number of features\n",
    "output_dim = 3  # Number of classes\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Bidirectional LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        # Adjusted to process the LSTM output at each timestep\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)  # Still doubling hidden_dim for bidirectional output\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply dropout\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Apply the fully connected layer to each timestep\n",
    "        # No need to concatenate the last hidden states from both directions\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "print_epochs = 1\n",
    "\n",
    "# Create the model with bidirectional LSTM\n",
    "model = BiLSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "model.to(device)  # Move your model to the MPS device\n",
    "\n",
    "# Loss function, optimizer, and scheduler remain the same\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=step_size, gamma=gamma)\n",
    "\n",
    "# hist will track the loss for now\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "# Ensure your model is in training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    batch_losses = []  # List to store batch losses\n",
    "    \n",
    "    # Training phase\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        # Move X_batch and y_batch to the correct device\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_train_pred = model(X_batch)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = 0\n",
    "        for timestep in range(y_train_pred.shape[1]):  # Iterate through each timestep\n",
    "            loss += loss_fn(y_train_pred[:, timestep], y_batch[:, timestep])\n",
    "        loss /= y_train_pred.shape[1]  # Average loss across timesteps\n",
    "        \n",
    "        # Backward pass and optimize\n",
    "        optimiser.zero_grad()\n",
    "        loss.backward()\n",
    "        optimiser.step()\n",
    "        \n",
    "        batch_losses.append(loss.item())\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    # Calculate average loss for the epoch\n",
    "    epoch_loss = np.mean(batch_losses)\n",
    "    hist[epoch] = epoch_loss\n",
    "    \n",
    "    if epoch % print_epochs == 0:  # Adjust logging frequency according to your preference\n",
    "        print(f\"Epoch {epoch}, Loss: {epoch_loss}\")\n",
    "        \n",
    "        # Optional: Add accuracy calculation or other metrics here\n",
    "        # Note: You'd typically calculate validation metrics here using a separate validation set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust the figure size\n",
    "# plt.figure(figsize=(6, 3))\n",
    "\n",
    "# # Plot the training loss\n",
    "# plt.plot(hist, label=\"Training loss\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor_gpu = X_test_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_tensor_gpu)\n",
    "    \n",
    "    # Convert logits to probabilities for each class\n",
    "    probabilities = torch.softmax(y_test_pred, dim=2)  # Assuming the model outputs logits with shape [batch_size, sequence_length, num_classes]\n",
    "\n",
    "    # Get the predicted class labels for each time step\n",
    "    _, predicted_labels = torch.max(probabilities, dim=2)\n",
    "\n",
    "    # Move the predictions back to CPU if needed, and convert to numpy for further processing or evaluation\n",
    "    predicted_labels_numpy = predicted_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(411520,)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_labels_flat = predicted_labels_numpy.flatten()\n",
    "predicted_labels_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = data.data['symbol'][-len(predicted_labels_numpy):].copy()\n",
    "df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "\n",
    "\n",
    "# signal = df_split['signal']\n",
    "# entries = signal == 2\n",
    "# exits = signal == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = vbt.Portfolio.from_signals(\n",
    "    close=df_split.Close, \n",
    "    long_entries=entries, \n",
    "    long_exits=exits,\n",
    "    size=100,\n",
    "    size_type='value',\n",
    "    init_cash='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 600\n",
    "vbt.settings['plotting']['layout']['height'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot({\"orders\", \"cum_returns\"}, settings=dict(bm_returns=False)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pf.stats()\n",
    "total_return = stats['Total Return [%]']\n",
    "orders = stats['Total Orders']\n",
    "print(\"Total Orders:\", orders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Convert tensors to numpy arrays for use with Scikit-Learn\n",
    "true_labels = y_test.cpu().numpy()\n",
    "pred_labels = predicted_labels.cpu().numpy()\n",
    "\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')  # 'macro' for unweighted mean\n",
    "recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print('Confusion Matrix:\\n', conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
