{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "import torch\n",
    "import math, time\n",
    "import itertools\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 800\n",
    "vbt.settings['plotting']['layout']['height'] = 400\n",
    "\n",
    "# #hparams\n",
    "timestep = 80\n",
    "# # Update these dimensions based on your dataset\n",
    "\n",
    "# hidden_dim = 32\n",
    "# num_layers = 2\n",
    "\n",
    "# num_epochs = 200\n",
    "# learning_rate=0.01\n",
    "# step_size=30\n",
    "# gamma=0.9\n",
    "\n",
    "# dropout_rate=0.2\n",
    "# print_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "df['signal'] = df['signal'].replace({'SignalNone': 1, 'SignalLong': 2, 'SignalShort': 0})\n",
    "df.fillna(method='pad');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)\n",
    "features = data.run(\"talib\", mavp=vbt.run_arg_dict(periods=14))\n",
    "data.data['symbol'] = pd.concat([data.data['symbol'], features], axis=1)\n",
    "data.data['symbol'].drop(['Open', 'High', 'Low'], axis=1, inplace=True)\n",
    "# This will drop columns from the DataFrame where all values are NaN\n",
    "data.data['symbol'] = data.data['symbol'].dropna(axis=1, how='all')\n",
    "\n",
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "data.data['symbol'] = data.data['symbol'].dropna()\n",
    "predictor_list = data.data['symbol'].drop('signal', axis=1).columns.tolist()\n",
    "\n",
    "\n",
    "X = data.data['symbol'][predictor_list]\n",
    "\n",
    "y = data.data['symbol']['signal']\n",
    "\n",
    "X.columns = X.columns.astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# First, split your data into a training+validation set and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.3)\n",
    "\n",
    "# Then, split the training+validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2)  # 0.2 here means 20% of the original data, or 25% of the training+validation set\n",
    "\n",
    "\n",
    "\n",
    "# Create a scaler instance\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler to your data and transform\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# Create a DataFrame from the scaled data with the same index and columns\n",
    "X_train_scaled_df = pd.DataFrame(X_train_scaled, index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "# Fit the scaler to your data and transform\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# Create a DataFrame from the scaled data with the same index and columns\n",
    "X_test_scaled_df = pd.DataFrame(X_test_scaled, index=X_test.index, columns=X_test.columns)\n",
    "\n",
    "# Fit the scaler to your data and transform\n",
    "X_val_scaled = scaler.fit_transform(X_val)\n",
    "\n",
    "# Create a DataFrame from the scaled data with the same index and columns\n",
    "X_val_scaled_df = pd.DataFrame(X_val_scaled, index=X_val.index, columns=X_val.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if np.array_equal(X_train_scaled_df.values, X_train_scaled):\n",
    "    print('Success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def create_sequences(input_data, sequence_length):\n",
    "    sequences = []\n",
    "    data_len = len(input_data)\n",
    "    for i in range(data_len - sequence_length):\n",
    "        seq = input_data[i:(i + sequence_length)]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "# Assuming X_train_scaled_df and X_test_scaled_df are already scaled and are DataFrames\n",
    "X_train_list = create_sequences(X_train_scaled_df.values, timestep)\n",
    "X_test_list = create_sequences(X_test_scaled_df.values, timestep)\n",
    "X_val_list = create_sequences(X_val_scaled_df.values, timestep)\n",
    "\n",
    "y_train_seq_ar = y_train[timestep:]\n",
    "y_test_seq_ar = y_test[timestep:]\n",
    "y_val_seq_ar = y_val[timestep:]\n",
    "\n",
    "\n",
    "x_train = np.array(X_train_list)\n",
    "x_test = np.array(X_test_list)  \n",
    "x_val = np.array(X_val_list)  \n",
    "\n",
    "y_train_seq = np.array(y_train_seq_ar)\n",
    "y_test_seq = np.array(y_test_seq_ar)\n",
    "y_val_seq = np.array(y_val_seq_ar)\n",
    "\n",
    "\n",
    "\n",
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move tensors to the MPS device\n",
    "x_train_tensor = torch.from_numpy(x_train).float().to(device)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_train_tensor = torch.from_numpy(y_train_seq).long().to(device)\n",
    "y_test_tensor = torch.from_numpy(y_test_seq).long().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "\n",
    "if isinstance(y_train_seq, torch.Tensor):\n",
    "    y_train_seq_np = y_train_seq.cpu().numpy()\n",
    "else:\n",
    "    y_train_seq_np = y_train_seq  # Assuming y_train_seq is already a numpy array or similar\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_seq_np), y=y_train_seq_np)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# Move class weights to the same device as your model and data\n",
    "class_weights_tensor = class_weights_tensor.to(device)  # device could be 'cpu' or 'cuda'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "num_epochs = 150\n",
    "input_dim = 175  # Number of features\n",
    "output_dim = 3  # Number of classes\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Bidirectional LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        # The input dimension is twice the hidden_dim because it's bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)  # times 2 for bidirectional\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)  # times 2 for bidirectional\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply dropout to the output of the LSTM\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Concatenate the hidden states from both directions\n",
    "        out = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        \n",
    "        # Pass the concatenated hidden states to the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Define the objective function that Optuna will optimize\n",
    "def objective(trial):\n",
    "    # Hyperparameters to be optimized by Optuna\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [16, 32, 64, 128])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    learning_rate = trial.suggest_loguniform('learning_rate', 1e-5, 1e-1)\n",
    "    step_size = trial.suggest_int('step_size', 10, 100)\n",
    "    gamma = trial.suggest_uniform('gamma', 0.85, 0.99)\n",
    "    dropout_rate = trial.suggest_uniform('dropout_rate', 0.1, 0.5)\n",
    "    \n",
    "    # Update the model with new hyperparameters\n",
    "    model = BiLSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "    model.to(device)\n",
    "    \n",
    "    # Convert y_train_seq and y_val_seq for the current trial\n",
    "    y_train_tensor = torch.from_numpy(y_train_seq).long().to(device)  # Convert y_train_seq\n",
    "    y_val_tensor = torch.from_numpy(y_val_seq).long().to(device)  # Convert y_val_seq\n",
    "    x_val_tensor = torch.from_numpy(x_val).float().to(device)  # Convert y_val_seq\n",
    "\n",
    "    # Loss function and optimizers\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(x_train)\n",
    "        loss = criterion(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Validation step\n",
    "        if epoch % 10 == 0:\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                val_output = model(x_val_tensor)  # You need to create a validation set 'x_val'\n",
    "                val_loss = criterion(val_output, y_val_tensor)  # You need to create a validation set 'y_val'\n",
    "            model.train()\n",
    "    \n",
    "    # Objective value to minimize (could be the validation loss)\n",
    "    return val_loss.item()\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)  # Specify the number of trials\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(f\"Best parameters: {best_params}\")\n",
    "\n",
    "# You can now retrain your model with the best parameters or analyze the results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust the figure size\n",
    "# plt.figure(figsize=(6, 3))\n",
    "\n",
    "# # Plot the training loss\n",
    "# plt.plot(hist, label=\"Training loss\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_pred = model(x_test)\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "    # Get the predicted class labels\n",
    "    _, predicted_labels = torch.max(probabilities, 1)\n",
    "    # Move the tensor to CPU and then convert to numpy\n",
    "    predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "    print(len(predicted_labels_numpy))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = data.data['symbol'][-len(predicted_labels_numpy):].copy()\n",
    "df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "signal = df_split['signal']\n",
    "entries = signal == 2\n",
    "exits = signal == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = vbt.Portfolio.from_signals(\n",
    "    close=df_split.Close, \n",
    "    long_entries=entries, \n",
    "    long_exits=exits,\n",
    "    size=100,\n",
    "    size_type='value',\n",
    "    init_cash='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot(settings=dict(bm_returns=False)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Convert tensors to numpy arrays for use with Scikit-Learn\n",
    "true_labels = y_test.cpu().numpy()\n",
    "pred_labels = predicted_labels.cpu().numpy()\n",
    "\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')  # 'macro' for unweighted mean\n",
    "recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print('Confusion Matrix:\\n', conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
