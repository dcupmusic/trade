{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook Instructions\n",
    "\n",
    "1. All the <u>code and data files</u> used in this course are available in the downloadable unit of the <u>last section of this course</u>.\n",
    "2. You can run the notebook document sequentially (one cell at a time) by pressing **shift + enter**. \n",
    "3. While a cell is running, a [*] is shown on the left. After the cell is run, the output will appear on the next line.\n",
    "\n",
    "This course is based on specific versions of python packages. You can find the details of the packages in <a href='https://quantra.quantinsti.com/quantra-notebook' target=\"_blank\" >this manual</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Weights in Decision Trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we are building a decision tree model, it can happen that the dataset provided to the model may have very few data points for it's most important classes. In such an instance, the decision tree algorithm will try to maximize the accuracy of the most common labels. \n",
    "\n",
    "In order to adjust for this issue, we re-assign weights to the data points of the most important labels. This can be done in the scikit-library using the class_weight argument to the decision tree classifier. Let us take an example to illustrate this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "\n",
    "We will input raw data of ACC Ltd. stock from a csv file. The data consists of Open-High-Low-Close prices and Volume data. Predictor and target variables are created using this raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data_modules/ACC.csv')\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Technical Indicators and Daily Future Returns\n",
    "\n",
    "We compute the values for the Average Directional Index (ADI), Relative Strength Index (RSI), and Simple Moving Average (SMA) using the TA-Lib package. These will be used as predictor variables in the decision tree model. Next, we compute the daily future returns on the close price. The code is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import talib as ta\n",
    "\n",
    "# Import and filter warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "df['ADX'] = ta.ADX(df['HIGH'].values, df['LOW'].values,\n",
    "                   df['CLOSE'].values, timeperiod=14)\n",
    "df['RSI'] = ta.RSI(df['CLOSE'].values, timeperiod=14)\n",
    "df['SMA'] = ta.SMA(df['CLOSE'].values, timeperiod=20)\n",
    "\n",
    "df['Return'] = df['CLOSE'].pct_change(1).shift(-1)\n",
    "df = df.dropna()\n",
    "\n",
    "df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Categorize Returns into Multiple Classes\n",
    "\n",
    "We define a function called 'returns_to_class' using nested If..else statement to categorize returns into multiple classes. We also specify the range for the returns for each class in this function. This function is then applied on our dataframe, df to get the multi-class target variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def returns_to_class(df):\n",
    "    if df.Return <= 0.0:\n",
    "        return 0\n",
    "    elif df.Return > 0.0 and df.Return < 0.02:\n",
    "        return 1\n",
    "    elif df.Return > 0.02 and df.Return < 0.03:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3\n",
    "\n",
    "\n",
    "df['Class'] = df.apply(returns_to_class, axis=1)\n",
    "df.tail(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View the Multi-Class Distribution\n",
    "\n",
    "Once we have defined the different classes for the target variable, we can see their distribution of Returns using the groupby method. As can be observed, out of the total data points majority of them (i.e. 126 data points) belong to '0' class which signifies negative returns. On the other hand, there are only 11 and 1 datapoint belonging to the '2' and the '3' class respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.groupby('Class').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Predictor Variables and Target Variable\n",
    "\n",
    "Let us now define our predictors variables, X and the target variable, y for building a decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['ADX', 'RSI', 'SMA']]\n",
    "y = df.Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  \n",
    "We will consider two scenarios:   \n",
    "\n",
    "1) Building a decision tree model without applying the class weights and    \n",
    "2) Building a decision tree model with class weights.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 1 - Build a decision tree model without applying the Class weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.97      0.85        29\n",
      "           1       0.50      0.12      0.20         8\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.74        39\n",
      "   macro avg       0.31      0.27      0.26        39\n",
      "weighted avg       0.67      0.74      0.67        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Split into Train and Test datasets\n",
    "split_percentage = 0.8\n",
    "split = int(split_percentage*len(X))\n",
    "# Train data set\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "# Test data set\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]\n",
    "\n",
    "#print (X_train.shape, y_train.shape)\n",
    "#print (X_test.shape, y_test.shape)\n",
    "\n",
    "# Fit a model on train data\n",
    "clf = DecisionTreeClassifier(criterion='entropy', max_depth=5, min_samples_leaf=5)\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)\n",
    "\n",
    "# Here, the warning is occurring because some labels in y_test don't appear in y_pred. In this case, the number of observations\n",
    "# for label 2 and label 3 are very few and they might not be occurring in the y_pred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen from the output of the classification report, the decision tree algorithm tries to maximize the accuracy of the most common labels and does not give good predictions on the underrepresented labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scenario 2 - Build a decision tree model with Class Weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the class_weight parameter when defining the decision tree classifier to correct for the underrepresented labels.\n",
    "\n",
    "We can assigned class_weight = 'balanced'. This re-weighting of the data points causes the classes to appear with equal frequency.\n",
    "\n",
    "As can be seen from the output of the classification report, using class weight makes the decision tree algorithm achieve higher accuracy on the underrepresented labels which were labels '2'and '3' in this case. Although after class weights are changed, a model which otherwise shows good performance can suddenly appear to be less effective."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        29\n",
      "           1       0.28      0.88      0.42         8\n",
      "           2       0.00      0.00      0.00         1\n",
      "           3       0.07      1.00      0.13         1\n",
      "\n",
      "    accuracy                           0.21        39\n",
      "   macro avg       0.09      0.47      0.14        39\n",
      "weighted avg       0.06      0.21      0.09        39\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Split into Train and Test datasets\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "split_percentage = 0.8\n",
    "split = int(split_percentage*len(X))\n",
    "# Train data set\n",
    "X_train = X[:split]\n",
    "y_train = y[:split]\n",
    "# Test data set\n",
    "X_test = X[split:]\n",
    "y_test = y[split:]\n",
    "\n",
    "# Fit a model on train data\n",
    "clf = DecisionTreeClassifier(criterion='gini', max_depth=3, min_samples_leaf=5,\n",
    "                             class_weight='balanced')\n",
    "clf = clf.fit(X_train, y_train)\n",
    "\n",
    "# Use the trained model to make predictions on the test data\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Evaluate the model performance\n",
    "report = classification_report(y_test, y_pred)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try this model yourself on a new dataset to see how it works. In the next unit, there will be an interactive exercise. All the best!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
