{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "import random\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Seed for Python's random module\n",
    "random.seed(42)\n",
    "\n",
    "# Seed for NumPy's random number generator\n",
    "np.random.seed(42)\n",
    "\n",
    "# Seed for PyTorch\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Additional settings for multi-threaded reproducibility\n",
    "torch.use_deterministic_algorithms(True)  # This is a newer way compared to 'torch.backends.cudnn.deterministic'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df_filtered = df[df['signal'] != 'SignalNone']\n",
    "\n",
    "# Iterate through the DataFrame and adjust the signals\n",
    "for i in range(1, len(df_filtered)):\n",
    "    current_signal = df_filtered.iloc[i]['signal']\n",
    "    previous_signal = df_filtered.iloc[i - 1]['signal']\n",
    "    current_close = df_filtered.iloc[i]['Close']\n",
    "    previous_close = df_filtered.iloc[i - 1]['Close']\n",
    "    \n",
    "    if current_signal == previous_signal:\n",
    "        if current_signal == 'SignalLong' and previous_close > current_close:\n",
    "            df_filtered.iloc[i - 1, df_filtered.columns.get_loc('signal')] = 'SignalNone'\n",
    "        elif current_signal != 'SignalLong' and previous_close < current_close:\n",
    "            df_filtered.iloc[i - 1, df_filtered.columns.get_loc('signal')] = 'SignalNone'\n",
    "        else:\n",
    "            df_filtered.iloc[i, df_filtered.columns.get_loc('signal')] = 'SignalNone'\n",
    "\n",
    "\n",
    "df.update(df_filtered)\n",
    "\n",
    "# # Assuming df is your DataFrame\n",
    "# previous_signal = None  # Initialize a variable to keep track of the previous non-\"SignalNone\" value\n",
    "\n",
    "# for i in range(len(df)):\n",
    "#     if df.iloc[i, df_filtered.columns.get_loc('signal')] == \"SignalNone\" and previous_signal is not None:\n",
    "#         df.iloc[i, df_filtered.columns.get_loc('signal')] = previous_signal  # Replace \"SignalNone\" with the previous signal\n",
    "#     elif df.iloc[i, df_filtered.columns.get_loc('signal')] != \"SignalNone\":\n",
    "#         previous_signal = df.iloc[i, df_filtered.columns.get_loc('signal')]  # Update the previous signal to the current one if it's not \"SignalNone\"\n",
    "\n",
    "# df = df.loc[df['signal'] != 'SignalNone']\n",
    "\n",
    "df['signal'] = df['signal'].replace({'SignalLong': 2, 'SignalShort': 0, 'SignalNone': 1})\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)\n",
    "features = data.run(\"talib\", mavp=vbt.run_arg_dict(periods=14))\n",
    "data.data['symbol'] = pd.concat([data.data['symbol'], features], axis=1)\n",
    "data.data['symbol'].drop(['Open', 'High', 'Low'], axis=1, inplace=True)\n",
    "# This will drop columns from the DataFrame where all values are NaN\n",
    "data.data['symbol'] = data.data['symbol'].dropna(axis=1, how='all')\n",
    "\n",
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "data.data['symbol'] = data.data['symbol'].dropna()\n",
    "predictor_list = data.data['symbol'].drop('signal', axis=1).columns.tolist()\n",
    "\n",
    "\n",
    "X = data.data['symbol'][predictor_list]\n",
    "\n",
    "y = data.data['symbol']['signal']\n",
    "\n",
    "X.columns = X.columns.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set\n",
    "# Assuming X is a DataFrame or a NumPy array\n",
    "indices = np.arange(X.shape[0])\n",
    "\n",
    "# First, split your data into a training+validation set and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test, indices_train_val, indices_test = train_test_split(X, y, indices, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Then, split the training+validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(X_train_val, y_train_val, indices_train_val, test_size=0.2, shuffle=False)  # 0.2 here means 20% of the original data, or 25% of the training+validation set\n",
    "\n",
    "# Now, `indices_val` holds the indices of your original dataset that were used for the validation set.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 20\n",
    "\n",
    "def create_sequences(input_data, timestep):\n",
    "    sequences = []\n",
    "    data_len = len(input_data)\n",
    "    for i in range(data_len - timestep):\n",
    "        seq = input_data[i:(i + timestep)]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "X_train_list = create_sequences(X_train_scaled, timestep)\n",
    "X_val_list = create_sequences(X_val_scaled, timestep)\n",
    "X_test_list = create_sequences(X_test_scaled, timestep)\n",
    "y_train_seq_ar = y_train[timestep:]\n",
    "y_val_seq_ar = y_val[timestep:]\n",
    "y_test_seq_ar = y_test[timestep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "x_train_ar = np.array(X_train_list)\n",
    "y_train_seq = np.array(y_train_seq_ar).astype(int)\n",
    "x_val_ar = np.array(X_val_list)  \n",
    "y_val_seq = np.array(y_val_seq_ar).astype(int)\n",
    "x_test_ar = np.array(X_test_list)  \n",
    "y_test_seq = np.array(y_test_seq_ar).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(x_train_ar, dtype=torch.float32) # .to(device)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(x_val_ar, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(x_test_ar, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "if isinstance(y_train_seq, torch.Tensor):\n",
    "    y_train_seq_np = y_train_seq.cpu().numpy()\n",
    "else:\n",
    "    y_train_seq_np = y_train_seq  # Assuming y_train_seq is already a numpy array or similar\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_seq_np), y=y_train_seq_np)\n",
    "# Decrease the weight of the '1' class\n",
    "decrease_factor = 0.1  # Adjust this factor as needed\n",
    "class_weights[1] *= decrease_factor\n",
    "\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# Move class weights to the same device as your model and data\n",
    "class_weights_tensor = class_weights_tensor.to(device)  # device could be 'cpu' or 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output shape: [batch_size, seq_length, hidden_dim]\n",
    "        weights = torch.tanh(self.linear(lstm_output))\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        \n",
    "        # Context vector with weighted sum\n",
    "        context = weights * lstm_output\n",
    "        context = torch.sum(context, dim=1)\n",
    "        return context, weights\n",
    "\n",
    "class BiLSTMClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate):\n",
    "        super(BiLSTMClassifierWithAttention, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Batch Normalization Layer for Conv1d\n",
    "        self.bn_conv1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = Attention(hidden_dim * 2)  # For bidirectional LSTM\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # Adjusted for attention context vector\n",
    "        \n",
    "        # Batch Normalization Layer for FC1\n",
    "        self.bn_fc1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
    "        \n",
    "        # Additional Dropout for the fully connected layer\n",
    "        self.dropout_fc = nn.Dropout(dropout_rate / 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape x for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn_conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Reshape back for LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Applying attention mechanism to LSTM outputs\n",
    "        context, _ = self.attention(out)\n",
    "        \n",
    "        # Fully connected layers using the context vector from attention\n",
    "        out = self.fc1(context)\n",
    "        out = self.bn_fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout_fc(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_multi_with_metrics(model, criterion, X_val, y_val, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    with torch.no_grad():\n",
    "        output = model(X_val)\n",
    "        predicted_probs = torch.softmax(output, dim=1)\n",
    "        predictions = torch.argmax(predicted_probs, dim=1)  # Get the index of the max log-probability as the prediction\n",
    "        \n",
    "        loss = criterion(output, y_val)  # Ensure y_val is of dtype long and contains class indices\n",
    "\n",
    "        # Convert to CPU and numpy for sklearn metrics\n",
    "        predictions_np = predictions.cpu().numpy()\n",
    "        y_val_np = y_val.cpu().numpy()\n",
    "\n",
    "        accuracy = accuracy_score(y_val_np, predictions_np)\n",
    "        precision = precision_score(y_val_np, predictions_np, average='weighted')\n",
    "        recall = recall_score(y_val_np, predictions_np, average='weighted')\n",
    "        f1 = f1_score(y_val_np, predictions_np, average='weighted')\n",
    "\n",
    "    model.train()  # Set back to train mode\n",
    "    return {\n",
    "        'loss': round(loss.item(), 4),\n",
    "        'accuracy': round(accuracy, 4),\n",
    "        'precision': round(precision, 4),\n",
    "        'recall': round(recall, 4),\n",
    "        'f1': round(f1, 4)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_financials(model, X_val_selected_gpu):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_val_selected_gpu)\n",
    "        probabilities = torch.sigmoid(y_test_pred).squeeze()  # Apply sigmoid to convert logits to probabilities\n",
    "        predicted_labels = (probabilities > 0.5).long()  # Threshold probabilities to get binary predictions\n",
    "        predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Use predicted labels to simulate a trading strategy\n",
    "    adjusted_indices_val = indices_val[timestep:]\n",
    "    adjusted_indices_test = indices_test[timestep:] \n",
    "    \n",
    "    df_split = data.data['symbol'].iloc[adjusted_indices_val].copy()\n",
    "    df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "    signal = df_split['signal']\n",
    "    entries = signal == 1\n",
    "    exits = signal == 0\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=df_split.Close, \n",
    "        long_entries=entries, \n",
    "        short_entries=exits,\n",
    "        size=100,\n",
    "        size_type='value',\n",
    "        init_cash='auto'\n",
    "    )\n",
    "    stats = pf.stats()\n",
    "    total_return = stats['Total Return [%]']\n",
    "    orders = stats['Total Orders']\n",
    "    calmer_ratio = stats['Calmar Ratio']\n",
    "    \n",
    "    model.train()\n",
    "    return {\n",
    "        \"orders\": orders,\n",
    "        \"calmer_returns\": (calmer_ratio+total_return)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from collections import deque\n",
    "\n",
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 500\n",
    "vbt.settings['plotting']['layout']['height'] = 250\n",
    "\n",
    "def objective(trial, type, hparams):\n",
    "\n",
    "\n",
    "    if hparams == 'static':\n",
    "        # Suggest hyperparameters\n",
    "        hidden_dim = 32 # trial.suggest_categorical('hidden_dim', [16, 32, 64])\n",
    "        num_layers = 2 # trial.suggest_int('num_layers', 1, 3)\n",
    "        lr = 1e-2 # trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "        step_size = 25 # trial.suggest_int('step_size', 10, 100)\n",
    "        gamma = 0.85 # trial.suggest_float('gamma', 0.85, 0.99)\n",
    "        dropout_rate = 0.1 # trial.suggest_float('dropout_rate', 0.1, 0.4)\n",
    "    elif hparams == 'dynamic':\n",
    "        # Suggest hyperparameters\n",
    "        hidden_dim = trial.suggest_categorical('hidden_dim', [16, 32, 64])\n",
    "        num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "        lr = trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "        step_size = trial.suggest_int('step_size', 10, 100)\n",
    "        gamma = trial.suggest_float('gamma', 0.85, 0.99)\n",
    "        dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.4)\n",
    "\n",
    "    if type == 'single':\n",
    "        # Use only the selected feature to create new tensors\n",
    "        feature_idx = 89 # trial.suggest_int('feature_idx', 0, X_train_tensor.shape[2] - 1)\n",
    "        X_train_selected = X_train_tensor[:, :, 0:X_train_tensor.shape[2] - 1]\n",
    "        X_val_selected = X_val_tensor[:, :, 0:X_train_tensor.shape[2] - 1]\n",
    "        # X_test_selected = X_test_tensor[:, :, feature_idx:feature_idx+1]\n",
    "    elif type == 'tophalf':\n",
    "        # Use only the selected feature to create new tensors\n",
    "        feature_idx = trial.suggest_categorical('feature_idx', unique_features_0)\n",
    "        X_train_selected = X_train_tensor[:, :, feature_idx:feature_idx+1]\n",
    "        X_val_selected = X_val_tensor[:, :, feature_idx:feature_idx+1]\n",
    "    elif type == 'multi':\n",
    "        # Suggest a boolean flag for each feature to decide if it should be included\n",
    "        included_features = [trial.suggest_categorical(f'include_feature_{i}', [True, False]) for i in unique_features]\n",
    "        included_features_idx = [i for i, f in enumerate(included_features) if f]\n",
    "        # If no features are selected, we can either skip this trial or select a default feature\n",
    "        if not included_features_idx:\n",
    "            return None  # Or handle this case as you see fit\n",
    "        # Use a selection of features to create new tensors\n",
    "        X_train_selected = X_train_tensor[:, :, included_features_idx]\n",
    "        X_val_selected = X_val_tensor[:, :, included_features_idx]\n",
    "        X_test_selected = X_test_tensor[:, :, included_features_idx]\n",
    "    elif type == 'all':\n",
    "        # Use only the selected features to create new tensors\n",
    "        X_train_selected = X_train_tensor[:, :, top_group_feature_indices]\n",
    "        X_val_selected = X_val_tensor[:, :, top_group_feature_indices]\n",
    "        X_test_selected = X_test_tensor[:, :, top_group_feature_indices] \n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    X_train_selected_gpu = X_train_selected.float().to(device)\n",
    "    X_val_selected_gpu = X_val_selected.float().to(device)\n",
    "    y_train_tensor_gpu = y_train_tensor.float().to(device)\n",
    "    y_val_tensor_gpu = y_val_tensor.float().to(device)\n",
    "\n",
    "    # Initialize model and move it to the MPS device\n",
    "    model = BiLSTMClassifierWithAttention(input_dim=X_train_selected.shape[-1], hidden_dim=hidden_dim, num_layers=num_layers, output_dim=len(np.unique(y_train_tensor.cpu().numpy())), dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "\n",
    "\n",
    "    rolling_window = deque(maxlen=rolling_window_size)\n",
    "    epoch_nums_1 = []\n",
    "    calmer_return = []\n",
    "    val_loss = []\n",
    "    accuracy = []\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    # auc = []\n",
    "    \n",
    "    \n",
    "    \n",
    "    best_calmer_return = float('-inf')  # Initialize with the lowest possible number\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # use a small number of epochs for demonstration\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_selected_gpu)\n",
    "        output = torch.squeeze(output)  # This removes the extra dimension\n",
    "        loss = criterion(output, y_train_tensor_gpu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        if epoch % validation_frequency == 0:\n",
    "            # financial_results = validate_financials(model, X_val_selected_gpu)\n",
    "            # current_calmer_return = financial_results['calmer_returns']\n",
    "            validation_results = validate_multi_with_metrics(model, criterion, X_val_selected_gpu, y_val_tensor_gpu, device)\n",
    "\n",
    "        # # Append metrics for plotting\n",
    "            epoch_nums_1.append(epoch)\n",
    "            val_loss.append(validation_results['loss'])\n",
    "            accuracy.append(validation_results['accuracy'])\n",
    "            precision.append(validation_results['precision'])\n",
    "            recall.append(validation_results['recall'])\n",
    "            f1.append(validation_results['f1'])\n",
    "            # auc.append(validation_results['auc'])\n",
    "            # calmer_return.append(current_calmer_return)\n",
    "        \n",
    "            # Append the new return to the rolling window\n",
    "            # rolling_window.append(current_calmer_return)\n",
    "\n",
    "            # if len(rolling_window) == rolling_window_size:\n",
    "            #     variance = np.var(list(rolling_window))\n",
    "                \n",
    "            #     if hparams == 'dynamic':\n",
    "            #         # Save the model if the current return is a new maximum\n",
    "            #         if current_calmer_return > best_calmer_return:\n",
    "            #             best_calmer_return = current_calmer_return\n",
    "            #             # Save the model\n",
    "            #             torch.save(model.state_dict(), f'model_epoch_{epoch}_calmer_{current_calmer_return:.4f}.pth')\n",
    "            #             print(f\"New best model saved at epoch {epoch} with Calmer Return: {current_calmer_return}\")\n",
    "                    \n",
    "            #     # Check if the variance is below the threshold\n",
    "            #     if variance < variance_threshold:\n",
    "            #         print(f\"Early stopping triggered at epoch {epoch}. Variance of returns over the last {rolling_window_size} validation steps is below the threshold.\")\n",
    "            #         break\n",
    "                \n",
    "            #     # Check if the last rolling_window_size returns are negative\n",
    "            #     if len(calmer_return) >= rolling_window_size and all(x < 0 for x in calmer_return[-rolling_window_size:]):\n",
    "            #         print(f\"Early stopping triggered at epoch {epoch}. The last {rolling_window_size} returns are negative.\")\n",
    "            #         break\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_val_selected_gpu)\n",
    "        probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "        _, predicted_labels = torch.max(probabilities, 1)\n",
    "        predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Use predicted labels to simulate a trading strategy\n",
    "    adjusted_indices_val = indices_val[timestep:]\n",
    "    adjusted_indices_test = indices_test[timestep:] \n",
    "    df_split = data.data['symbol'].iloc[adjusted_indices_val].copy()\n",
    "\n",
    "\n",
    "    df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "    signal = df_split['signal']\n",
    "    entries = signal == 2\n",
    "    exits = signal == 0\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=df_split.Close, \n",
    "        long_entries=entries, \n",
    "        short_entries=exits,\n",
    "        size=100,\n",
    "        size_type='value',\n",
    "        init_cash='auto'\n",
    "    )\n",
    "    \n",
    "    stats = pf.stats()\n",
    "    total_return = round(stats['Total Return [%]'], 2)\n",
    "    orders = stats['Total Orders']\n",
    "    calmer_ratio = stats['Calmar Ratio']\n",
    "    \n",
    "\n",
    "    pf_bt_fig = pf.plot({\"orders\", \"cum_returns\"})\n",
    "    validation_metrics_fig = go.Figure()\n",
    "\n",
    "    validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=val_loss, mode='lines+markers', name='val_loss'))\n",
    "    validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=accuracy, mode='lines+markers', name='accuracy'))\n",
    "    validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=precision, mode='lines+markers', name='precision'))\n",
    "    validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=recall, mode='lines+markers', name='recall'))\n",
    "    validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=f1, mode='lines+markers', name='f1'))\n",
    "    # validation_metrics_fig.add_trace(go.Scatter(x=epoch_nums_1, y=auc, mode='lines+markers', name='auc'))\n",
    "\n",
    "    validation_metrics_fig.update_layout(\n",
    "        template='plotly_dark',\n",
    "        autosize=False,\n",
    "        width=700,  # Set the width of the figure\n",
    "        height=150,  # Set the height of the figure\n",
    "        margin=dict(l=10, r=10, b=10, t=30, pad=5),\n",
    "        title='Calmer Ratio + Returns Over Epochs'  # You can set the title directly here\n",
    "    )\n",
    "\n",
    "\n",
    "    # if orders < 6:\n",
    "    #     print(f\"Only {orders} trades were made\")\n",
    "    #     calmer_ratio = 0.0\n",
    "    #     total_return = 0.0\n",
    "    # else:\n",
    "    #     if (calmer_ratio + total_return) > 0:\n",
    "    pf_bt_fig.show()    \n",
    "    validation_metrics_fig.show()\n",
    "    print(f\"Total returns: {total_return} %\")\n",
    "        \n",
    "        \n",
    "    # # Return the total return as the objective to maximize it\n",
    "    return round((calmer_ratio + total_return), 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before running the study, ensure your data tensors are on the CPU as Optuna will handle moving them to the GPU\n",
    "\n",
    "X_train_tensor = X_train_tensor.cpu()\n",
    "y_train_tensor = y_train_tensor.cpu()\n",
    "X_val_tensor = X_val_tensor.cpu()\n",
    "y_val_tensor = y_val_tensor.cpu()\n",
    "X_test_tensor = X_test_tensor.cpu()\n",
    "y_test_tensor = y_test_tensor.cpu()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 100\n",
    "validation_frequency = 10 # how often (epochs) to run validation\n",
    "\n",
    "rolling_window_size = 15 # how many validation freqs to consider for variance\n",
    "variance_threshold = 3 # below this threshold, we stop training\n",
    "\n",
    "num_trials_0 = 5 # X_train_tensor.shape[2] * 1.2  # number of trials to run\n",
    "\n",
    "study_0 = optuna.create_study(direction='maximize')\n",
    "study_0.optimize(lambda trial: objective(trial, \"single\", \"static\"), n_trials=num_trials_0)\n",
    "\n",
    "print('Best trial:', study_0.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_trials_0 = [trial for trial in study_0.trials if trial.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "# Sort the trials based on their performance (assuming higher return is better)\n",
    "\n",
    "sorted_trials_0 = sorted(completed_trials_0, key=lambda trial: trial.value, reverse=True)\n",
    "\n",
    "# Get the top N performing feature indices\n",
    "top_n_0 = int(len(completed_trials_0)/2)  # For example, top 5 features\n",
    "top_n_features_0 = [trial.params['feature_idx'] for trial in sorted_trials_0[:top_n_0]]\n",
    "\n",
    "# # print(\"Top performing feature indices:\", top_n_features)\n",
    "\n",
    "# Map the indices to names\n",
    "top_n_feature_names_0 = [predictor_list[idx] for idx in top_n_features_0]\n",
    "print(f\"Top performing features: {top_n_feature_names_0}\")\n",
    "print(f\"Top performing indicies: {top_n_features_0}\")\n",
    "\n",
    "# Remove duplicates without preserving order\n",
    "unique_features_0 = list(set(top_n_features_0))\n",
    "\n",
    "print(unique_features_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials_1 = len(unique_features_0) * 1.3  # number of trials to run\n",
    "\n",
    "study_1 = optuna.create_study(direction='maximize')\n",
    "study_1.optimize(lambda trial: objective(trial, \"tophalf\", \"static\"), n_trials=num_trials_1)\n",
    "\n",
    "print('Best trial:', study_1.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_trials = [trial for trial in study_1.trials if trial.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "\n",
    "sorted_trials = sorted(completed_trials, key=lambda trial: trial.value, reverse=True)\n",
    "\n",
    "# Get the top N performing feature indices\n",
    "top_n = 20  # For example, top 5 features\n",
    "top_n_features = [trial.params['feature_idx'] for trial in sorted_trials[:top_n]]\n",
    "\n",
    "# # print(\"Top performing feature indices:\", top_n_features)\n",
    "\n",
    "# Map the indices to names\n",
    "top_n_feature_names = [predictor_list[idx] for idx in top_n_features]\n",
    "print(f\"Top performing features: {top_n_feature_names}\")\n",
    "print(f\"Top performing indicies: {top_n_features}\")\n",
    "\n",
    "# Remove duplicates without preserving order\n",
    "unique_features = list(set(top_n_features))\n",
    "\n",
    "print(unique_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials_2 = 100  # number of trials to run\n",
    "\n",
    "study_2 = optuna.create_study(direction='maximize')\n",
    "study_2.optimize(lambda trial: objective(trial, \"multi\", \"static\"), n_trials=num_trials_2)\n",
    "\n",
    "print('Best trial:', study_2.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_2 = study_2.best_trial\n",
    "\n",
    "print(f\"Best trial number: {best_trial_2.number}\")\n",
    "print(\"Best trial's parameters:\", best_trial_2.params)\n",
    "print(\"Best trial's objective value:\", best_trial_2.value)\n",
    "\n",
    "# Assuming best_trial.params is your dictionary\n",
    "params_2 = best_trial_2.params\n",
    "\n",
    "# Extracting feature indices for which the value is True\n",
    "top_group_feature_indices = [int(key.split('_')[-1]) for key, value in params_2.items() if value]\n",
    "\n",
    "print(\"Included feature indices:\", top_group_feature_indices)\n",
    "\n",
    "# Map the indices to names\n",
    "top_group_feature_names = [predictor_list[idx] for idx in top_group_feature_indices]\n",
    "\n",
    "print(\"Top performing feature names:\", top_group_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials_3 = 2\n",
    "\n",
    "study_3 = optuna.create_study(direction='maximize')\n",
    "study_3.optimize(lambda trial: objective(trial, \"all\", \"dynamic\"), n_trials=num_trials_3)\n",
    "\n",
    "print(f'Best trial:', study_3.best_trial.number, study_3.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial_3 = study_3.best_trial\n",
    "\n",
    "print(f\"Best trial number: {best_trial_3.number}\")\n",
    "print(\"Best trial's parameters:\", best_trial_3.params)\n",
    "print(\"Best trial's objective value:\", best_trial_3.value)\n",
    "\n",
    "# Assuming best_trial.params is your dictionary\n",
    "params_3 = best_trial_3.params\n",
    "params_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = X_train_tensor.cpu()\n",
    "y_train_tensor = y_train_tensor.cpu()\n",
    "X_val_tensor = X_val_tensor.cpu()\n",
    "y_val_tensor = y_val_tensor.cpu()\n",
    "X_test_tensor = X_test_tensor.cpu()\n",
    "y_test_tensor = y_test_tensor.cpu()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
