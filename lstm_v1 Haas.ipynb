{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "fc7bf9a0-e26b-4de5-8db7-7a410a9c31d1",
    "_uuid": "91e741e3-0b5a-4d10-a22e-fab5924337e5"
   },
   "source": [
    "In this notebook we will be building and training LSTM to predict IBM stock. We will use PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "import math, time\n",
    "import itertools\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 800\n",
    "vbt.settings['plotting']['layout']['height'] = 400\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Performance-test_2.csv')\n",
    "# df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "# df.set_index('Timestamp', inplace=True)\n",
    "\n",
    "df['Signal'] = df['Signal'].replace({'SignalNone': 1, 'SignalLong': 2, 'SignalShort': 0})\n",
    "\n",
    "df.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.copy()\n",
    "\n",
    "X = data.iloc[:, :-1]\n",
    "\n",
    "y = data.iloc[:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(0.3*(len(X)))\n",
    "X_train = X[:-test_size]\n",
    "X_test = X[-test_size:]\n",
    "\n",
    "y_train = y[:-test_size]\n",
    "y_test = y[-test_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "\n",
    "X_train_scaled=scl.fit_transform(X_train)\n",
    "X_train= X_train.assign(Timestamp=X_train_scaled[:, 0])\n",
    "X_train= X_train.assign(Price=X_train_scaled[:, 1])\n",
    "X_train= X_train.assign(RSI=X_train_scaled[:, 2])\n",
    "\n",
    "\n",
    "\n",
    "X_test_scaled=scl.transform(X_test)\n",
    "\n",
    "X_test= X_test.assign(Timestamp=X_test_scaled[:, 0])\n",
    "X_test= X_test.assign(Price=X_test_scaled[:, 1])\n",
    "X_test= X_test.assign(RSI=X_test_scaled[:, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 20\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "# Adjust the range to stop at the last point where a full timestep can be created\n",
    "for i in range(timestep, len(X_train) - timestep + 1):  # Adjust the loop to stop earlier\n",
    "    X_train_list.append(np.array(X_train.iloc[i-timestep:i]))\n",
    "    # Only append the next value instead of a range of values\n",
    "    y_train_list.append(y_train.iloc[i])  # Assuming you want the next value as the target\n",
    "\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for i in range(timestep, len(X_test) - timestep + 1):  # Adjust the loop to stop earlier\n",
    "    X_test_list.append(np.array(X_test.iloc[i-timestep:i]))\n",
    "    # Only append the next value instead of a range of values\n",
    "    y_test_list.append(y_test.iloc[i])  # Assuming you want the next value as the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(X_train_list)\n",
    "x_test = np.array(X_test_list)  \n",
    "\n",
    "y_train = np.array(y_train_list)\n",
    "y_test = np.array(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training and test sets in torch\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming y_train is your target labels tensor for the training data\n",
    "# and it's already in the form of a 1D tensor of class indices (0 to C-1)\n",
    "\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = y_train  # Assuming y_train is already a numpy array\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move class weights to the same device as your model and data\n",
    "class_weights_tensor = class_weights_tensor.to('cpu')  # device could be 'cpu' or 'cuda'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Update these dimensions based on your dataset\n",
    "input_dim = X.shape[1]\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 3  # Number of classes\n",
    "\n",
    "# Create the model\n",
    "model = LSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim)\n",
    "\n",
    "# Use CrossEntropyLoss for multi-class classification\n",
    "# Initialize the loss function with class weights\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Optimizer\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.05)\n",
    "\n",
    "# Assuming `optimizer` is your optimizer (e.g., Adam)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=10, gamma=0.8)\n",
    "\n",
    "\n",
    "# Print the model's architecture\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "\n",
    "\n",
    "model.train()\n",
    "for t in range(num_epochs):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_train_pred = model(x_train)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_train_pred, y_train.long())  # Ensure y_train is of type torch.long\n",
    "    if t % 20 == 0:\n",
    "        print(\"Epoch \", t, \"Loss: \", loss.item())\n",
    "\n",
    "    # Zero gradients before backward pass\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Perform backward pass: compute gradients of the loss with respect to all the learnable parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters using the gradients and optimizer algorithm\n",
    "    optimiser.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    \n",
    "    y_test_pred = model(x_test)\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "\n",
    "    # Get the predicted class labels\n",
    "    _, predicted_labels = torch.max(probabilities, 1)\n",
    "predicted_labels_numpy = predicted_labels.numpy()\n",
    "\n",
    "\n",
    "df_split = data[-len(predicted_labels_numpy):].copy()\n",
    "df_split.loc[:, \"Signal\"] = predicted_labels_numpy\n",
    "signal = df_split['Signal']\n",
    "entries = signal == 2\n",
    "exits = signal == 0\n",
    "pf = vbt.Portfolio.from_signals(\n",
    "    close=df_split.Price, \n",
    "    long_entries=entries, \n",
    "    long_exits=exits,\n",
    "    size=100,\n",
    "    size_type='value',\n",
    "    init_cash='auto'\n",
    ")\n",
    "\n",
    "print(pf.stats()[\"Total Return [%]\"])\n",
    "pf.plot({\"orders\", \"cum_returns\", }, settings=dict(bm_returns=False)).show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
