{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "import math, time\n",
    "import itertools\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "df['signal'] = df['signal'].replace({'SignalNone': 1, 'SignalLong': 2, 'SignalShort': 0})\n",
    "df.fillna(method='pad')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### indicators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "adx = vbt.ADX.run(high_price, low_price, close_price, window=14)\n",
    "atr = vbt.ATR.run(high_price, low_price, close_price, window=14)\n",
    "bbands = vbt.BBANDS.run(close_price, window=14)\n",
    "rsi = vbt.RSI.run(close_price)\n",
    "sma = vbt.MA.run(close_price, window=20)\n",
    "strend = vbt.SUPERTREND.run(high_price, low_price, close_price, period=7, multiplier=3)\n",
    "\n",
    "data.data['symbol']['ADX'] = adx.adx\n",
    "data.data['symbol']['ATR'] = atr.atr\n",
    "data.data['symbol']['BBAND'] = bbands.bandwidth\n",
    "data.data['symbol']['RSI'] = rsi.rsi\n",
    "data.data['symbol']['SMA'] = sma.ma\n",
    "data.data['symbol']['STREND'] = strend.trend\n",
    "data.data['symbol'] = data.data['symbol'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_list = ['Open', 'High', 'Low', 'Close', 'Volume', 'ADX', 'ATR', 'BBAND', 'RSI', 'SMA', 'STREND']\n",
    "\n",
    "X = data.data['symbol'][predictor_list]\n",
    "\n",
    "y = data.data['symbol']['signal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = int(0.3*(len(X)))\n",
    "X_train = X[:-test_size]\n",
    "X_test = X[-test_size:]\n",
    "\n",
    "y_train = y[:-test_size]\n",
    "y_test = y[-test_size:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scl = StandardScaler()\n",
    "\n",
    "X_train_scaled=scl.fit_transform(X_train)\n",
    "X_train= X_train.assign(Open=X_train_scaled[:, 0])\n",
    "X_train= X_train.assign(High=X_train_scaled[:, 1])\n",
    "X_train= X_train.assign(Low=X_train_scaled[:, 2])\n",
    "X_train= X_train.assign(Close=X_train_scaled[:, 3])\n",
    "X_train= X_train.assign(Volume=X_train_scaled[:, 4])\n",
    "X_train= X_train.assign(ADX=X_train_scaled[:, 5])\n",
    "X_train= X_train.assign(ATR=X_train_scaled[:, 6])\n",
    "X_train= X_train.assign(BBAND=X_train_scaled[:, 7])\n",
    "X_train= X_train.assign(RSI=X_train_scaled[:, 8])\n",
    "X_train= X_train.assign(SMA=X_train_scaled[:, 9])\n",
    "X_train= X_train.assign(STREND=X_train_scaled[:, 10])\n",
    "\n",
    "\n",
    "X_test_scaled=scl.transform(X_test)\n",
    "\n",
    "X_test= X_test.assign(Open=X_test_scaled[:, 0])\n",
    "X_test= X_test.assign(High=X_test_scaled[:, 1])\n",
    "X_test= X_test.assign(Low=X_test_scaled[:, 2])\n",
    "X_test= X_test.assign(Close=X_test_scaled[:, 3])\n",
    "X_test= X_test.assign(Volume=X_test_scaled[:, 4])\n",
    "X_test= X_test.assign(ADX=X_test_scaled[:, 5])\n",
    "X_test= X_test.assign(ATR=X_test_scaled[:, 6])\n",
    "X_test= X_test.assign(BBAND=X_test_scaled[:, 7])\n",
    "X_test= X_test.assign(RSI=X_test_scaled[:, 8])\n",
    "X_test= X_test.assign(SMA=X_test_scaled[:, 9])\n",
    "X_test= X_test.assign(STREND=X_test_scaled[:, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 80\n",
    "X_train_list = []\n",
    "y_train_list = []\n",
    "\n",
    "# Adjust the range to stop at the last point where a full timestep can be created\n",
    "for i in range(timestep, len(X_train) - timestep + 1):  # Adjust the loop to stop earlier\n",
    "    X_train_list.append(np.array(X_train.iloc[i-timestep:i]))\n",
    "    # Only append the next value instead of a range of values\n",
    "    y_train_list.append(y_train.iloc[i])  # Assuming you want the next value as the target\n",
    "\n",
    "X_test_list = []\n",
    "y_test_list = []\n",
    "\n",
    "for i in range(timestep, len(X_test) - timestep + 1):  # Adjust the loop to stop earlier\n",
    "    X_test_list.append(np.array(X_test.iloc[i-timestep:i]))\n",
    "    # Only append the next value instead of a range of values\n",
    "    y_test_list.append(y_test.iloc[i])  # Assuming you want the next value as the target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = np.array(X_train_list)\n",
    "x_test = np.array(X_test_list)  \n",
    "\n",
    "y_train = np.array(y_train_list)\n",
    "y_test = np.array(y_test_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make training and test sets in torch\n",
    "x_train = torch.from_numpy(x_train).type(torch.Tensor)\n",
    "x_test = torch.from_numpy(x_test).type(torch.Tensor)\n",
    "y_train = torch.from_numpy(y_train).long()\n",
    "y_test = torch.from_numpy(y_test).long()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### class weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming y_train is your target labels tensor for the training data\n",
    "# and it's already in the form of a 1D tensor of class indices (0 to C-1)\n",
    "\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = y_train  # Assuming y_train is already a numpy array\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "# decrease_factor = 0.2  # Adjust this factor as needed\n",
    "# class_weights[1] *= decrease_factor\n",
    "# Convert class weights to a tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move class weights to the same device as your model and data\n",
    "# class_weights_tensor = class_weights_tensor.to('cpu')  # device could be 'cpu' or 'cuda'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Take the output of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Update these dimensions based on your dataset\n",
    "input_dim = 11  # Number of features\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "output_dim = 3  # Number of classes\n",
    "\n",
    "# Create the model\n",
    "model = LSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim).to(device)\n",
    "\n",
    "# Use CrossEntropyLoss for multi-class classification\n",
    "# Initialize the loss function with class weights\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Optimizer\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Assuming `optimizer` is your optimizer (e.g., Adam)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=10, gamma=0.9)\n",
    "\n",
    "\n",
    "# Print the model's architecture\n",
    "# print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_vals(hist):\n",
    "    # Create a subplot with 2 rows and 1 column\n",
    "\n",
    "    fig = go.Figure()\n",
    "    # Add validation loss trace to the first row\n",
    "    fig.add_trace(go.Scatter(x=list(range(len(hist))), y=hist, mode='lines+markers', name='val_loss'))\n",
    "    \n",
    "    # Update layout for the combined figure\n",
    "    fig.update_layout(\n",
    "        template='plotly_dark',\n",
    "        autosize=False,\n",
    "        width=700,  # Adjust the width of the figure\n",
    "        height=300,  # Adjust the height of the figure (make it larger to accommodate both subplots)\n",
    "        title_text='loss over epochs',\n",
    "        title_font_size=10,\n",
    "        margin=dict(l=5, r=5, b=5, t=30, pad=5),\n",
    "        legend=dict(\n",
    "            font=dict(\n",
    "                size=5)\n",
    "        )\n",
    "    )\n",
    "    # Show the combined figure\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 1000\n",
    "# Assuming loss_fn is already defined as CrossEntropyLoss\n",
    "# e.g., loss_fn = torch.nn.CrossEntropyLoss()\n",
    "# For classification, it might be more informative to track accuracy or other metrics\n",
    "# hist will track the loss for now\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "for t in range(num_epochs):\n",
    "    x_train_gpu = x_train.to(device)\n",
    "    y_train_gpu = y_train.long().to(device)\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_train_pred = model(x_train_gpu)\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(y_train_pred, y_train_gpu)  # Ensure y_train is of type torch.long\n",
    "    if t % 20 == 0:  # Adjust logging frequency according to your preference\n",
    "        print(\"Epoch \", t, \"Loss: \", loss.item())\n",
    "    hist[t] = loss.item()\n",
    "\n",
    "    # Zero gradients before backward pass\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Perform backward pass: compute gradients of the loss with respect to all the learnable parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # Update the parameters using the gradients and optimizer algorithm\n",
    "    optimiser.step()\n",
    "    \n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "\n",
    "    # Optional: Calculate and print accuracy or other metrics every few epochs\n",
    "    # This is more meaningful for classification tasks\n",
    "    if t % 20 == 0:  # Adjust as needed\n",
    "        model.eval()\n",
    "        with torch.no_grad():  # No need to track gradients for validation\n",
    "            y_pred_tags = torch.argmax(torch.softmax(y_train_pred, dim=1), dim=1)\n",
    "            correct_preds = (y_pred_tags == y_train_gpu).float().sum()\n",
    "            accuracy = correct_preds / y_train.shape[0]\n",
    "            print(f'Epoch {t} Accuracy: {accuracy.item() * 100:.2f}%')\n",
    "        model.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    x_test_gpu = x_test.to(device)\n",
    "    y_test_pred = model(x_test_gpu)\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "\n",
    "    # Get the predicted class labels\n",
    "    _, predicted_labels = torch.max(probabilities, 1)\n",
    "predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "len(predicted_labels_numpy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### backtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = data.data['symbol'][-len(predicted_labels_numpy):].copy()\n",
    "df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "signal = df_split['signal']\n",
    "entries = signal == 2\n",
    "exits = signal == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = vbt.Portfolio.from_signals(\n",
    "    close=df_split.Close, \n",
    "    long_entries=entries, \n",
    "    long_exits=exits,\n",
    "    size=100,\n",
    "    size_type='value',\n",
    "    init_cash='auto'\n",
    ")\n",
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 700\n",
    "vbt.settings['plotting']['layout']['height'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_vals(hist)\n",
    "pf.plot({\"orders\", \"cum_returns\"}, settings=dict(bm_returns=False)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"epochs: {num_epochs}\")\n",
    "print(f\"returns: {round(pf.stats()['Total Return [%]'], 2)}%\")\n",
    "print(f\"loss: {round(hist[len(hist)-1], 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### valuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "# Convert tensors to numpy arrays for use with Scikit-Learn\n",
    "true_labels = y_test.cpu().numpy()\n",
    "pred_labels = predicted_labels.cpu().numpy()\n",
    "\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')  # 'macro' for unweighted mean\n",
    "recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "ConfusionMatrixDisplay(conf_matrix).plot();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
