{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df['signal'] = df['signal'].replace({'SignalNone': 1, 'SignalLong': 2, 'SignalShort': 0})\n",
    "df = df.ffill()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)\n",
    "features = data.run(\"talib\", mavp=vbt.run_arg_dict(periods=14))\n",
    "data.data['symbol'] = pd.concat([data.data['symbol'], features], axis=1)\n",
    "data.data['symbol'].drop(['Open', 'High', 'Low'], axis=1, inplace=True)\n",
    "# This will drop columns from the DataFrame where all values are NaN\n",
    "data.data['symbol'] = data.data['symbol'].dropna(axis=1, how='all')\n",
    "\n",
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "data.data['symbol'] = data.data['symbol'].dropna()\n",
    "predictor_list = data.data['symbol'].drop('signal', axis=1).columns.tolist()\n",
    "\n",
    "\n",
    "X = data.data['symbol'][predictor_list]\n",
    "\n",
    "y = data.data['symbol']['signal']\n",
    "\n",
    "X.columns = X.columns.astype(str)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training set and a test set\n",
    "# Assuming X is a DataFrame or a NumPy array\n",
    "indices = np.arange(X.shape[0])\n",
    "\n",
    "# First, split your data into a training+validation set and a separate test set\n",
    "X_train_val, X_test, y_train_val, y_test, indices_train_val, indices_test = train_test_split(X, y, indices, test_size=0.3, shuffle=False)\n",
    "\n",
    "# Then, split the training+validation set into a training set and a validation set\n",
    "X_train, X_val, y_train, y_val, indices_train, indices_val = train_test_split(X_train_val, y_train_val, indices_train_val, test_size=0.2, shuffle=False)  # 0.2 here means 20% of the original data, or 25% of the training+validation set\n",
    "\n",
    "# Now, `indices_val` holds the indices of your original dataset that were used for the validation set.\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "X_val_scaled = scaler.fit_transform(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestep = 20\n",
    "\n",
    "def create_sequences(input_data, timestep):\n",
    "    sequences = []\n",
    "    data_len = len(input_data)\n",
    "    for i in range(data_len - timestep):\n",
    "        seq = input_data[i:(i + timestep)]\n",
    "        sequences.append(seq)\n",
    "    return np.array(sequences)\n",
    "\n",
    "X_train_list = create_sequences(X_train_scaled, timestep)\n",
    "X_val_list = create_sequences(X_val_scaled, timestep)\n",
    "X_test_list = create_sequences(X_test_scaled, timestep)\n",
    "y_train_seq_ar = y_train[timestep:]\n",
    "y_val_seq_ar = y_val[timestep:]\n",
    "y_test_seq_ar = y_test[timestep:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to numpy arrays\n",
    "x_train_ar = np.array(X_train_list)\n",
    "y_train_seq = np.array(y_train_seq_ar).astype(int)\n",
    "x_val_ar = np.array(X_val_list)  \n",
    "y_val_seq = np.array(y_val_seq_ar).astype(int)\n",
    "x_test_ar = np.array(X_test_list)  \n",
    "y_test_seq = np.array(y_test_seq_ar).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to tensors\n",
    "X_train_tensor = torch.tensor(x_train_ar, dtype=torch.float32) # .to(device)\n",
    "y_train_tensor = torch.tensor(y_train_seq, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(x_val_ar, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val_seq, dtype=torch.long)\n",
    "X_test_tensor = torch.tensor(x_test_ar, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test_seq, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "# print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "if isinstance(y_train_seq, torch.Tensor):\n",
    "    y_train_seq_np = y_train_seq.cpu().numpy()\n",
    "else:\n",
    "    y_train_seq_np = y_train_seq  # Assuming y_train_seq is already a numpy array or similar\n",
    "\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_seq_np), y=y_train_seq_np)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "\n",
    "# Move class weights to the same device as your model and data\n",
    "class_weights_tensor = class_weights_tensor.to(device)  # device could be 'cpu' or 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.linear = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output shape: [batch_size, seq_length, hidden_dim]\n",
    "        weights = torch.tanh(self.linear(lstm_output))\n",
    "        weights = F.softmax(weights, dim=1)\n",
    "        \n",
    "        # Context vector with weighted sum\n",
    "        context = weights * lstm_output\n",
    "        context = torch.sum(context, dim=1)\n",
    "        return context, weights\n",
    "\n",
    "class BiLSTMClassifierWithAttention(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate):\n",
    "        super(BiLSTMClassifierWithAttention, self).__init__()\n",
    "        \n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Convolutional Layer\n",
    "        self.conv1 = nn.Conv1d(in_channels=input_dim, out_channels=hidden_dim, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Batch Normalization Layer for Conv1d\n",
    "        self.bn_conv1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        # LSTM Layer\n",
    "        self.lstm = nn.LSTM(hidden_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Attention Layer\n",
    "        self.attention = Attention(hidden_dim * 2)  # For bidirectional LSTM\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(hidden_dim * 2, hidden_dim)  # Adjusted for attention context vector\n",
    "        \n",
    "        # Batch Normalization Layer for FC1\n",
    "        self.bn_fc1 = nn.BatchNorm1d(hidden_dim)\n",
    "        \n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)  # Output layer\n",
    "        \n",
    "        # Additional Dropout for the fully connected layer\n",
    "        self.dropout_fc = nn.Dropout(dropout_rate / 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Reshape x for Conv1d\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Convolutional layer\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn_conv1(x)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        # Reshape back for LSTM\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        # Initialize hidden and cell states\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # LSTM layer\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Applying attention mechanism to LSTM outputs\n",
    "        context, _ = self.attention(out)\n",
    "        \n",
    "        # Fully connected layers using the context vector from attention\n",
    "        out = self.fc1(context)\n",
    "        out = self.bn_fc1(out)\n",
    "        out = F.relu(out)\n",
    "        out = self.dropout_fc(out)\n",
    "        out = self.fc2(out)\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 500\n",
    "vbt.settings['plotting']['layout']['height'] = 250\n",
    "\n",
    "num_epochs = 30\n",
    "num_trials = 5 # X_train_tensor.shape[2]  # number of trials to run\n",
    "# lets validate our technical indicators with the signal\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_dim = 32 # trial.suggest_categorical('hidden_dim', [16, 32, 64])\n",
    "    num_layers = 2 # trial.suggest_int('num_layers', 1, 3)\n",
    "    lr = 1e-3 # trial.suggest_float('lr', 1e-5, 1e-1, log=True)\n",
    "    step_size = 25 # trial.suggest_int('step_size', 10, 100)\n",
    "    gamma = 0.85 # trial.suggest_float('gamma', 0.85, 0.99)\n",
    "    dropout_rate = 0.1 # trial.suggest_float('dropout_rate', 0.1, 0.4)\n",
    "\n",
    "    ''' multi feature selection'''\n",
    "    # Suggest a boolean flag for each feature to decide if it should be included\n",
    "    num_features = X_train_tensor.shape[2]  # assuming the last dimension is the feature dimension\n",
    "    included_features = [trial.suggest_categorical(f'include_feature_{i}', [True, False]) for i in range(num_features)]\n",
    "    included_features_idx = [i for i, f in enumerate(included_features) if f]\n",
    "    # If no features are selected, we can either skip this trial or select a default feature\n",
    "    if not included_features_idx:\n",
    "        return None  # Or handle this case as you see fit\n",
    "    # Use a selection of features to create new tensors\n",
    "    X_train_selected = X_train_tensor[:, :, included_features_idx]\n",
    "    X_val_selected = X_val_tensor[:, :, included_features_idx]\n",
    "    X_test_selected = X_test_tensor[:, :, included_features_idx]\n",
    "    \n",
    "    ''' single feature selection'''\n",
    "    # Use only the selected feature to create new tensors\n",
    "    # feature_idx = trial.suggest_int('feature_idx', 0, X_train_tensor.shape[2] - 1)\n",
    "    # X_train_selected = X_train_tensor[:, :, feature_idx:feature_idx+1]\n",
    "    # X_val_selected = X_val_tensor[:, :, feature_idx:feature_idx+1]\n",
    "    # X_test_selected = X_test_tensor[:, :, feature_idx:feature_idx+1]\n",
    "\n",
    " \n",
    "    \n",
    "    # Move the selected feature tensors to the GPU\n",
    "    y_train_tensor_gpu = y_train_tensor.to(device)\n",
    "    X_train_selected_gpu = X_train_selected.to(device)\n",
    "    y_train_tensor_gpu = y_train_tensor.to(device)\n",
    "    X_val_selected_gpu = X_val_selected.to(device)\n",
    "\n",
    "    # Initialize model and move it to the MPS device\n",
    "    model = BiLSTMClassifierWithAttention(input_dim=X_train_selected.shape[-1], hidden_dim=hidden_dim, num_layers=num_layers, output_dim=len(np.unique(y_train_tensor.cpu().numpy())), dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "\n",
    "    # Training loop\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):  # use a small number of epochs for demonstration\n",
    "        optimizer.zero_grad()\n",
    "        output = model(X_train_selected_gpu)\n",
    "        loss = criterion(output, y_train_tensor_gpu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        # # Validation step\n",
    "        # if epoch % 10 == 0:\n",
    "        #     model.eval()\n",
    "        #     with torch.no_grad():\n",
    "        #         val_output = model(X_val_selected_gpu)\n",
    "        #         val_loss = criterion(val_output, y_val_tensor_gpu)\n",
    "        #         # Convert model outputs to predicted classes\n",
    "        #         _, predicted_classes = torch.max(val_output, 1)\n",
    "                \n",
    "        #         # Convert tensors to numpy arrays for compatibility with sklearn\n",
    "        #         predicted_classes = predicted_classes.cpu().numpy()\n",
    "        #         true_classes = y_val_tensor_gpu.cpu().numpy()\n",
    "                \n",
    "        #         # Filter out 'hold' predictions and labels\n",
    "        #         buy_sell_filter = (true_classes != 1) & (predicted_classes != 1)\n",
    "        #         filtered_true_classes = true_classes[buy_sell_filter]\n",
    "        #         filtered_predicted_classes = predicted_classes[buy_sell_filter]\n",
    "                \n",
    "        #         if len(filtered_predicted_classes) > 0 and len(filtered_true_classes) > 0:\n",
    "        #             accuracy = accuracy_score(filtered_true_classes, filtered_predicted_classes)\n",
    "        #             # print(f\"Validation Accuracy (excluding 'hold'): {accuracy}\")\n",
    "        #         else:\n",
    "        #             # print(\"Filtered classes are empty. Skipping accuracy calculation.\")\n",
    "        #             accuracy = 0  # or some default value\n",
    "                \n",
    "        #     model.train()\n",
    "\n",
    "    # return val_loss.item()\n",
    "        \n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model(X_val_selected_gpu)\n",
    "        probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "        _, predicted_labels = torch.max(probabilities, 1)\n",
    "        predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "    \n",
    "    # Use predicted labels to simulate a trading strategy\n",
    "    adjusted_indices_val = indices_val[timestep:]\n",
    "    adjusted_indices_test = indices_test[timestep:] \n",
    "    df_split = data.data['symbol'].iloc[adjusted_indices_val].copy()\n",
    "\n",
    "\n",
    "    df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "    signal = df_split['signal']\n",
    "    entries = signal == 2\n",
    "    exits = signal == 0\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=df_split.Close, \n",
    "        long_entries=entries, \n",
    "        long_exits=exits,\n",
    "        size=100,\n",
    "        size_type='value',\n",
    "        init_cash='auto'\n",
    "    )\n",
    "    stats = pf.stats()\n",
    "    total_return = stats['Total Return [%]']\n",
    "    orders = stats['Total Orders']\n",
    "    # max_drawdown = stats['Max Drawdown [%]']\n",
    "    # sortino_ratio = stats['Sortino Ratio']\n",
    "    calmer_ratio = stats['Calmar Ratio']\n",
    "\n",
    "    if orders < 8:\n",
    "        print(f\"Only {orders} trades were made\")\n",
    "        calmer_ratio = 0.0\n",
    "    else:\n",
    "        pf.plot({\"orders\", \"cum_returns\"}).show()\n",
    "        print(f\"Total returns: {total_return} %\")\n",
    "\n",
    "    # Return the loss as the objective to minimize it\n",
    "    # return accuracy\n",
    "    \n",
    "    # # Return the total return as the objective to maximize it\n",
    "    return calmer_ratio + total_return\n",
    "\n",
    "\n",
    "\n",
    "# Before running the study, ensure your data tensors are on the CPU as Optuna will handle moving them to the GPU\n",
    "X_train_tensor = X_train_tensor.cpu()\n",
    "y_train_tensor = y_train_tensor.cpu()\n",
    "X_val_tensor = X_val_tensor.cpu()\n",
    "y_val_tensor = y_val_tensor.cpu()\n",
    "X_test_tensor = X_test_tensor.cpu()\n",
    "y_test_tensor = y_test_tensor.cpu()\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=num_trials)\n",
    "\n",
    "print('Best trial:', study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'study' is your completed Optuna study\n",
    "# Get all completed trials\n",
    "completed_trials = [trial for trial in study.trials if trial.state == optuna.trial.TrialState.COMPLETE]\n",
    "\n",
    "\n",
    "# Get all completed trials\n",
    "# completed_trials = study.trials\n",
    "# completed_trials\n",
    "\n",
    "# # Sort the trials based on their performance (assuming higher return is better)\n",
    "# # Note: Adjust the sorting key based on your actual return metric if necessary\n",
    "sorted_trials = sorted(completed_trials, key=lambda trial: trial.value, reverse=True)\n",
    "\n",
    "# Get the top N performing feature indices\n",
    "top_n = 5  # For example, top 5 features\n",
    "top_n_features = [trial.params['feature_idx'] for trial in sorted_trials[:top_n]]\n",
    "\n",
    "# # print(\"Top performing feature indices:\", top_n_features)\n",
    "\n",
    "# Map the indices to names\n",
    "top_performing_feature_names = [predictor_list[idx] for idx in top_n_features]\n",
    "top_performing_feature_names\n",
    "# top_n_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'study' is your completed Optuna study\n",
    "\n",
    "\n",
    "best_trial = study.best_trial\n",
    "\n",
    "print(f\"Best trial number: {best_trial.number}\")\n",
    "print(\"Best trial's parameters:\", best_trial.params)\n",
    "print(\"Best trial's objective value:\", best_trial.value)\n",
    "\n",
    "\n",
    "# Assuming best_trial.params is your dictionary\n",
    "params = best_trial.params\n",
    "\n",
    "# Extracting feature indices for which the value is True\n",
    "included_feature_indices = [int(key.split('_')[-1]) for key, value in params.items() if value]\n",
    "\n",
    "print(\"Included feature indices:\", included_feature_indices)\n",
    "\n",
    "# Map the indices to names\n",
    "top_performing_feature_names = [predictor_list[idx] for idx in included_feature_indices]\n",
    "\n",
    "print(\"Top performing feature names:\", top_performing_feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_trials_2 = 5\n",
    "num_epochs_2 = 10\n",
    "\n",
    "\n",
    "def objective_2(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_dim = trial.suggest_categorical('hidden_dim', [16, 32, 64])\n",
    "    num_layers = trial.suggest_int('num_layers', 1, 3)\n",
    "    lr = trial.suggest_float('lr', 1e-4, 1e-2, log=True)\n",
    "    step_size = trial.suggest_int('step_size', 10, 100)\n",
    "    gamma = trial.suggest_float('gamma', 0.85, 0.99)\n",
    "    dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.4)\n",
    "    # feature_idx = trial.suggest_int('feature_idx', 0, X_train_tensor.shape[2] - 1)\n",
    "    # Suggest a boolean flag for each feature to decide if it should be included\n",
    "    # num_features = X_train_tensor.shape[2]  # assuming the last dimension is the feature dimension\n",
    "    # included_features = [trial.suggest_categorical(f'include_top_feature_{i}', [True, False]) for i in top_n_features]\n",
    "\n",
    "    # included_features_idx = [i for i, f in enumerate(included_features) if f]\n",
    "    \n",
    "    # If no features are selected, we can either skip this trial or select a default feature\n",
    "    # if not included_features_idx:\n",
    "    #     return None  # Or handle this case as you see fit\n",
    "    \n",
    "    \n",
    "    # Use only the selected features to create new tensors\n",
    "    X_train_selected = X_train_tensor[:, :, included_feature_indices] # top_n_features]\n",
    "    X_val_selected = X_val_tensor[:, :, included_feature_indices] # top_n_features]\n",
    "    X_test_selected = X_test_tensor[:, :, included_feature_indices] # top_n_features]\n",
    "\n",
    "    # Initialize model and move it to the MPS device\n",
    "    model_2 = BiLSTMClassifierWithAttention(input_dim=X_train_selected.shape[-1], hidden_dim=hidden_dim, num_layers=num_layers, output_dim=len(np.unique(y_train_tensor.cpu().numpy())), dropout_rate=dropout_rate).to(device)\n",
    "    optimizer = optim.Adam(model_2.parameters(), lr=lr)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=step_size, gamma=gamma)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "    X_train_tensor_gpu = X_train_tensor.to(device)\n",
    "    y_train_tensor_gpu = y_train_tensor.to(device)\n",
    "    X_val_tensor_gpu = X_val_tensor.to(device)\n",
    "    y_val_tensor_gpu = y_val_tensor.to(device)\n",
    "    X_test_tensor_gpu = X_test_tensor.to(device)\n",
    "    y_test_tensor_gpu = y_test_tensor.to(device)\n",
    "    \n",
    "    # Move the selected feature tensors to the GPU\n",
    "    X_train_selected_gpu = X_train_selected.to(device)\n",
    "    y_train_tensor_gpu = y_train_tensor.to(device)\n",
    "    X_test_selected_gpu = X_test_selected.to(device)\n",
    "    X_val_selected_gpu = X_val_selected.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    model_2.train()\n",
    "    for epoch in range(num_epochs_2):  # use a small number of epochs for demonstration\n",
    "        optimizer.zero_grad()\n",
    "        output = model_2(X_train_selected_gpu)\n",
    "        loss = criterion(output, y_train_tensor_gpu)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        # # Validation step\n",
    "        if epoch % 20 == 0:\n",
    "            print(f\"Epoch {epoch} loss: {loss.item()}\")\n",
    "        \n",
    "        \n",
    "    model_2.eval()\n",
    "    with torch.no_grad():\n",
    "        y_test_pred = model_2(X_test_selected_gpu) # (X_test_selected_gpu)\n",
    "        probabilities = torch.softmax(y_test_pred, dim=1)\n",
    "        _, predicted_labels = torch.max(probabilities, 1)\n",
    "        predicted_labels_numpy = predicted_labels.cpu().numpy()\n",
    "\n",
    "    # Use predicted labels to simulate a trading strategy\n",
    "    adjusted_indices_val = indices_val[timestep:]\n",
    "    adjusted_indices_test = indices_test[timestep:] \n",
    "    df_split = data.data['symbol'].iloc[adjusted_indices_test].copy()\n",
    "    \n",
    "    df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "    signal = df_split['signal']\n",
    "    entries = signal == 2\n",
    "    exits = signal == 0\n",
    "    pf = vbt.Portfolio.from_signals(\n",
    "        close=df_split.Close, \n",
    "        long_entries=entries, \n",
    "        long_exits=exits,\n",
    "        size=100,\n",
    "        size_type='value',\n",
    "        init_cash='auto'\n",
    "    )\n",
    "\n",
    "    stats = pf.stats()\n",
    "    total_return = stats['Total Return [%]']\n",
    "    orders = stats['Total Orders']\n",
    "    calmer_ratio = stats['Calmar Ratio']\n",
    "\n",
    "    if orders < 10:\n",
    "        print(f\"Only {orders} trades were made\")\n",
    "        calmer_ratio = 0.0\n",
    "    else:\n",
    "        pf.plot({\"orders\", \"cum_returns\"}, settings=dict(bm_returns=False)).show()\n",
    "    \n",
    "    # Return the negative total return as the objective to maximize it\n",
    "    return calmer_ratio\n",
    "\n",
    "# Before running the study, ensure your data tensors are on the CPU as Optuna will handle moving them to the GPU\n",
    "X_train_tensor = X_train_tensor.cpu()\n",
    "y_train_tensor = y_train_tensor.cpu()\n",
    "X_val_tensor = X_val_tensor.cpu()\n",
    "y_val_tensor = y_val_tensor.cpu()\n",
    "X_test_tensor = X_test_tensor.cpu()\n",
    "y_test_tensor = y_test_tensor.cpu()\n",
    "\n",
    "study_2 = optuna.create_study(direction='maximize')\n",
    "study_2.optimize(objective_2, n_trials=num_trials_2)\n",
    "\n",
    "print(f'Best trial:', study_2.best_trial.number, study_2.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'study_2' is your completed Optuna study\n",
    "\n",
    "\n",
    "best_trial_2 = study_2.best_trial\n",
    "\n",
    "print(f\"Best trial number: {best_trial_2.number}\")\n",
    "print(\"Best trial's parameters:\", best_trial_2.params)\n",
    "print(\"Best trial's objective value:\", best_trial_2.value)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Assuming best_trial.params is your dictionary\n",
    "params_2 = best_trial_2.params\n",
    "params_2\n",
    "\n",
    "\n",
    "# Extracting feature indices for which the value is True\n",
    "# included_feature_indices = [int(key.split('_')[-1]) for key, value in params_2.items() if value]\n",
    "\n",
    "# print(\"Included feature indices:\", included_feature_indices)\n",
    "\n",
    "# # Map the indices to names\n",
    "# top_performing_feature_names = [predictor_list[idx] for idx in included_feature_indices]\n",
    "\n",
    "# print(\"Top performing feature names:\", top_performing_feature_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
