{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import pandas as pd \n",
    "from pylab import mpl, plt\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "mpl.rcParams['font.family'] = 'serif'\n",
    "%matplotlib inline\n",
    "\n",
    "import math, time\n",
    "import itertools\n",
    "import datetime\n",
    "from operator import itemgetter\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from math import sqrt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import vectorbtpro as vbt\n",
    "\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", UserWarning)\n",
    "\n",
    "\n",
    "# #hparams\n",
    "\n",
    "# # Update these dimensions based on your dataset\n",
    "\n",
    "hidden_dim = 32\n",
    "num_layers = 2\n",
    "\n",
    "num_epochs = 5\n",
    "learning_rate=0.01\n",
    "step_size=30\n",
    "gamma=0.9\n",
    "\n",
    "dropout_rate=0.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('2ySOLdata1h.csv')\n",
    "pd.set_option('future.no_silent_downcasting', True)\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'], unit='s')\n",
    "df.set_index('timestamp', inplace=True)\n",
    "\n",
    "df['signal'] = df['signal'].replace({'SignalNone': 1, 'SignalLong': 2, 'SignalShort': 0})\n",
    "df.ffill()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = vbt.Data.from_data(df)\n",
    "features = data.run(\"talib\", mavp=vbt.run_arg_dict(periods=14))\n",
    "data.data['symbol'] = pd.concat([data.data['symbol'], features], axis=1)\n",
    "data.data['symbol'].drop(['Open', 'High', 'Low'], axis=1, inplace=True)\n",
    "# This will drop columns from the DataFrame where all values are NaN\n",
    "data.data['symbol'] = data.data['symbol'].dropna(axis=1, how='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "open_price = data.get('Open')\n",
    "high_price = data.get('High')\n",
    "low_price = data.get('Low')\n",
    "close_price = data.get('Close')\n",
    "\n",
    "data.data['symbol'] = data.data['symbol'].dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the tuple column names and the 'signal' column\n",
    "columns_to_keep = data.data['symbol'].drop('signal', axis=1).columns.tolist()\n",
    "\n",
    "# Filter the DataFrame to keep only the specified columns\n",
    "filtered_df = data.data['symbol'][columns_to_keep]\n",
    "# Print the current column names to verify their format\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming filtered_df is your DataFrame and the last column is 'signal' or any categorical label\n",
    "\n",
    "# Select all columns except the last one for scaling\n",
    "columns_to_scale = filtered_df.columns[:-1]\n",
    "\n",
    "# Initialize the MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "# Fit and transform the data for the columns to scale\n",
    "# .values.reshape(-1, 1) is used to transform the data into the correct shape for scaling\n",
    "# Note: .fit_transform expects a 2D array, hence the reshaping is necessary\n",
    "scaled_columns.columns = scaled_columns.columns.astype(str)\n",
    "scaled_columns = scaler.fit_transform(filtered_df[columns_to_scale])\n",
    "\n",
    "# Create a DataFrame from the scaled columns\n",
    "scaled_df = pd.DataFrame(scaled_columns, index=filtered_df.index, columns=columns_to_scale)\n",
    "\n",
    "# Add the unscaled 'signal' column back to the scaled DataFrame\n",
    "scaled_df['signal'] = filtered_df['signal']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to create train, test data given stock data and sequence length\n",
    "def load_data(stock, look_back):\n",
    "    data_raw = stock.values # convert to numpy array\n",
    "    data = []\n",
    "    \n",
    "    # create all possible sequences of length look_back\n",
    "    for index in range(len(data_raw) - look_back): \n",
    "        data.append(data_raw[index: index + look_back])\n",
    "    \n",
    "    data = np.array(data);\n",
    "    test_set_size = int(np.round(0.2*data.shape[0]));\n",
    "    train_set_size = data.shape[0] - (test_set_size);\n",
    "    \n",
    "    x_train = data[:train_set_size,:-1,:-1]\n",
    "    y_train = data[:train_set_size,-1,-1]\n",
    "    \n",
    "    x_test = data[train_set_size:,:-1,:-1]\n",
    "    y_test = data[train_set_size:,-1,-1]\n",
    "    \n",
    "    return [x_train, y_train, x_test, y_test]\n",
    "\n",
    "look_back = 60 # choose sequence length\n",
    "x_train, y_train, x_test, y_test = load_data(filtered_df, look_back)\n",
    "print('x_train.shape = ',x_train.shape)\n",
    "print('x_test.shape = ',x_test.shape)\n",
    "print('y_train.shape = ',y_train.shape)\n",
    "print('y_test.shape = ',y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_numeric = x_train.astype(np.float32)\n",
    "x_test_numeric = x_test.astype(np.float32)\n",
    "y_train_numeric = y_train.astype(np.float32)\n",
    "y_test_numeric = y_test.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for MPS (GPU on M1 Mac) availability and set it as the device\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Assuming y_train is your target labels tensor for the training data\n",
    "# and it's already in the form of a 1D tensor of class indices (0 to C-1)\n",
    "\n",
    "# Convert y_train to a numpy array if it's a tensor\n",
    "if isinstance(y_train, torch.Tensor):\n",
    "    y_train_np = y_train.cpu().numpy()\n",
    "else:\n",
    "    y_train_np = y_train  # Assuming y_train is already a numpy array\n",
    "\n",
    "# Calculate class weights\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.unique(y_train_np), y=y_train_np)\n",
    "\n",
    "# Convert class weights to a tensor\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float)\n",
    "\n",
    "# Move class weights to the same device as your model and data\n",
    "class_weights_tensor = class_weights_tensor.to(device)  # device could be 'cpu' or 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Convert your numpy arrays to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(x_train_numeric, dtype=torch.float).to(device)\n",
    "y_train_tensor = torch.tensor(y_train_numeric, dtype=torch.long).to(device)  # Use torch.long for classification labels\n",
    "\n",
    "X_test_tensor = torch.tensor(x_test_numeric, dtype=torch.float).to(device)\n",
    "y_test_tensor = torch.tensor(y_test_numeric, dtype=torch.long).to(device)\n",
    "# Create TensorDatasets\n",
    "# train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "# test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "# # Create DataLoaders\n",
    "# batch_size = 20  # You can adjust the batch size\n",
    "# train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "# test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)  # Typically no need to shuffle the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "input_dim = X_train_tensor.shape[2]  # Number of features\n",
    "output_dim = 3  # Number of classes\n",
    "\n",
    "# Here we define our model as a class\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout_rate):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # Bidirectional LSTM Layer\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
    "        \n",
    "        # Dropout layer\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        \n",
    "        # Fully connected layer\n",
    "        # The input dimension is twice the hidden_dim because it's bidirectional\n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state and cell state\n",
    "        h0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)  # times 2 for bidirectional\n",
    "        c0 = torch.zeros(self.num_layers * 2, x.size(0), self.hidden_dim).to(x.device)  # times 2 for bidirectional\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, (hn, cn) = self.lstm(x, (h0, c0))\n",
    "        \n",
    "        # Apply dropout to the output of the LSTM\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        # Concatenate the hidden states from both directions\n",
    "        out = torch.cat((hn[-2,:,:], hn[-1,:,:]), dim = 1)\n",
    "        \n",
    "        # Pass the concatenated hidden states to the fully connected layer\n",
    "        out = self.fc(out)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Create the model with bidirectional LSTM\n",
    "model = BiLSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "model.to(device)  # Move your model to the MPS device\n",
    "\n",
    "# Loss function, optimizer, and scheduler remain the same\n",
    "loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "optimiser = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimiser, step_size=step_size, gamma=gamma)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 30\n",
    "print_epochs = 1\n",
    "\n",
    "# Create the model with bidirectional LSTM\n",
    "# model = BiLSTMClassifier(input_dim=input_dim, hidden_dim=hidden_dim, num_layers=num_layers, output_dim=output_dim, dropout_rate=dropout_rate)\n",
    "model.to(device)  # Move your model to the MPS device\n",
    "\n",
    "\n",
    "# hist will track the loss for now\n",
    "hist = np.zeros(num_epochs)\n",
    "\n",
    "# Ensure your model is in training mode\n",
    "model.train()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Initialise hidden state\n",
    "    # Don't do this if you want your LSTM to be stateful\n",
    "    #model.hidden = model.init_hidden()\n",
    "    \n",
    "    # Forward pass\n",
    "    y_train_pred = model(X_train_tensor)\n",
    "\n",
    "    loss = loss_fn(y_train_pred, y_train_tensor.long())\n",
    "\n",
    "    hist[epoch] = loss.item()\n",
    "\n",
    "    # Zero out gradient, else they will accumulate between epochs\n",
    "    optimiser.zero_grad()\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters\n",
    "    optimiser.step()\n",
    "    \n",
    "    scheduler.step()\n",
    " \n",
    "    \n",
    "    if epoch % print_epochs == 0 and epoch !=0:  # Adjust logging frequency according to your preference\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n",
    "        \n",
    "        # Optional: Add accuracy calculation or other metrics here\n",
    "        # Note: You'd typically calculate validation metrics here using a separate validation set\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Adjust the figure size\n",
    "# plt.figure(figsize=(6, 3))\n",
    "\n",
    "# # Plot the training loss\n",
    "# plt.plot(hist, label=\"Training loss\")\n",
    "# plt.legend()\n",
    "\n",
    "# # Show the plot\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_tensor_gpu = X_test_tensor.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y_test_pred = model(X_test_tensor_gpu)\n",
    "    \n",
    "    # Convert logits to probabilities for each class\n",
    "    probabilities = torch.softmax(y_test_pred, dim=2)  # Assuming the model outputs logits with shape [batch_size, sequence_length, num_classes]\n",
    "\n",
    "    # Get the predicted class labels for each time step\n",
    "    _, predicted_labels = torch.max(probabilities, dim=2)\n",
    "\n",
    "    # Move the predictions back to CPU if needed, and convert to numpy for further processing or evaluation\n",
    "    predicted_labels_numpy = predicted_labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_labels_flat = predicted_labels_numpy.flatten()\n",
    "predicted_labels_flat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split = data.data['symbol'][-len(predicted_labels_numpy):].copy()\n",
    "df_split.loc[:, \"signal\"] = predicted_labels_numpy\n",
    "\n",
    "\n",
    "# signal = df_split['signal']\n",
    "# entries = signal == 2\n",
    "# exits = signal == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf = vbt.Portfolio.from_signals(\n",
    "    close=df_split.Close, \n",
    "    long_entries=entries, \n",
    "    long_exits=exits,\n",
    "    size=100,\n",
    "    size_type='value',\n",
    "    init_cash='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vbt.settings.set_theme('dark')\n",
    "vbt.settings['plotting']['layout']['width'] = 600\n",
    "vbt.settings['plotting']['layout']['height'] = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.plot({\"orders\", \"cum_returns\"}, settings=dict(bm_returns=False)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pf.stats()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = pf.stats()\n",
    "total_return = stats['Total Return [%]']\n",
    "orders = stats['Total Orders']\n",
    "print(\"Total Orders:\", orders)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "\n",
    "# Convert tensors to numpy arrays for use with Scikit-Learn\n",
    "true_labels = y_test.cpu().numpy()\n",
    "pred_labels = predicted_labels.cpu().numpy()\n",
    "\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')  # 'macro' for unweighted mean\n",
    "recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "\n",
    "print(f'Precision: {precision:.2f}')\n",
    "print(f'Recall: {recall:.2f}')\n",
    "print(f'F1 Score: {f1:.2f}')\n",
    "print('Confusion Matrix:\\n', conf_matrix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
